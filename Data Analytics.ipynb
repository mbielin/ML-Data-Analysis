{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d760bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebaff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34257c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e991f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699d054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "856c5677",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35b998",
   "metadata": {},
   "source": [
    "The code is designed to train a linear regression model to predict the total quantity of lessons taken by past swim club members. The goal is to progressively remove data on the classes taken and evaluate how well the model can still predict the total lessons taken. The model is trained on past members, and the approach is based on class progression (i.e., sequence and quantity of classes taken).\n",
    "\n",
    "Key Steps:\n",
    "Data Preparation:\n",
    "\n",
    "The dataset is first split to focus on leavers (past members) who are no longer with the swim club. These leavers provide the historical data needed for training.\n",
    "A list of classes (PreSchool 1 to INTERMEDIATE) is used, representing the sequence of lessons available in the swim club.\n",
    "Feature Engineering:\n",
    "\n",
    "The Variance to Median (VTM) is calculated for each class column. This represents the difference between the number of classes taken by a member and the median number of classes taken by others, excluding zeros. This helps standardize the lesson progression across different members.\n",
    "Training on Reduced Class Data:\n",
    "\n",
    "The model is trained iteratively, each time using progressively fewer class data points. Starting with all the class data, three classes are removed in each iteration to see how well the model can still predict the total lessons taken when data is progressively withheld.\n",
    "The target variable is the Total Quantity of Lessons, and each iteration uses less and less class progression information to predict this target.\n",
    "Training and Testing:\n",
    "\n",
    "For each iteration, the code splits the data into training and testing sets (70% training, 30% testing).\n",
    "The features are scaled using a StandardScaler to normalize the data.\n",
    "A linear regression model is trained using the scaled data, and predictions are made on the test set.\n",
    "The model’s performance is evaluated using Mean Squared Error (MSE) and R-squared (a measure of how well the model fits the data).\n",
    "Performance Evaluation:\n",
    "\n",
    "The results are printed for each iteration, showing how the model’s performance changes as more class data is removed.\n",
    "Additionally, the actual and predicted total lessons are compared, showing how close the model's predictions are to the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6990a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "df = pd.read_excel('swimclass_rawdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9425254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes used: 14 | Mean Squared Error: 0.71 | R-squared: 1.00\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                38.997279   -0.002721\n",
      "30546                   105               104.934708   -0.065292\n",
      "20806                    35                34.975545   -0.024455\n",
      "39438                    30                30.002425    0.002425\n",
      "18229                   134               134.005573    0.005573\n",
      "Classes used: 11 | Mean Squared Error: 6.84 | R-squared: 1.00\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                39.014868    0.014868\n",
      "30546                   105               104.760644   -0.239356\n",
      "20806                    35                34.958702   -0.041298\n",
      "39438                    30                30.039038    0.039038\n",
      "18229                   134               133.944120   -0.055880\n",
      "Classes used: 8 | Mean Squared Error: 251.91 | R-squared: 0.91\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                41.100639    2.100639\n",
      "30546                   105               113.857982    8.857982\n",
      "20806                    35                39.207308    4.207308\n",
      "39438                    30                32.351213    2.351213\n",
      "18229                   134               130.209003   -3.790997\n",
      "Classes used: 5 | Mean Squared Error: 917.67 | R-squared: 0.66\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                55.097600   16.097600\n",
      "30546                   105               122.943163   17.943163\n",
      "20806                    35                57.916102   22.916102\n",
      "39438                    30                45.714584   15.714584\n",
      "18229                   134               134.471619    0.471619\n",
      "Classes used: 2 | Mean Squared Error: 2370.54 | R-squared: 0.11\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                40.192520    1.192520\n",
      "30546                   105                51.202079  -53.797921\n",
      "20806                    35                40.192520    5.192520\n",
      "39438                    30                40.192520   10.192520\n",
      "18229                   134               106.249875  -27.750125\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data for leavers (ignore current members for now)\n",
    "leavers_df = df[df['Current_member'] == 0].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Step 3: Define the class columns\n",
    "lesson_columns = ['PreSchool 1', 'PreSchool 2', 'PreSchool 3', 'PreSchool 4', 'PreSchool 5',\n",
    "                  'PreSchool 6', 'Academy 1', 'Academy 2', 'Academy 3', 'Academy 4', \n",
    "                  'Academy 5', 'Academy 6', 'BEGINNERS', 'INTERMEDIATE']\n",
    "\n",
    "# Step 4: Calculate Variance to Median (VTM) for each class column\n",
    "for col in lesson_columns:\n",
    "    median_value = leavers_df.loc[leavers_df[col] > 0, col].median()  # Exclude zero values and use .loc[]\n",
    "    leavers_df.loc[:, f'{col}_VTM'] = leavers_df[col] - median_value  # Explicit .loc[] usage to avoid the warning\n",
    "\n",
    "# Step 5: Train on progressively reduced class data\n",
    "for classes_used in range(len(lesson_columns), 1, -3):  # Remove class data progressively, 3 at a time\n",
    "    selected_classes = lesson_columns[:classes_used]\n",
    "    features = [f'{col}_VTM' for col in selected_classes] + ['TOTAL QUANTITY OF LESSONS']  # Exclude TOTAL_QUANTITY_OF_LESSONS\n",
    "    \n",
    "    # Step 6: Define the target (Total lessons we want to predict)\n",
    "    leavers_df.loc[:, 'PAST_LESSONS'] = leavers_df[lesson_columns].sum(axis=1)  # Known lessons taken so far\n",
    "    \n",
    "    # Remove TOTAL_QUANTITY_OF_LESSONS for training\n",
    "    X_leavers = leavers_df[features].drop(columns=['TOTAL QUANTITY OF LESSONS'])\n",
    "    y_leavers = leavers_df['TOTAL QUANTITY OF LESSONS']  # Total lessons as target\n",
    "    \n",
    "    # Step 7: Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Step 8: Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 9: Train the linear regression model\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Step 10: Make predictions and evaluate the model\n",
    "    y_pred = linear_reg.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results for each iteration\n",
    "    print(f\"Classes used: {classes_used} | Mean Squared Error: {mse:.2f} | R-squared: {r2:.2f}\")\n",
    "    \n",
    "    # Optional: Compare actual vs predicted total lessons\n",
    "    results_df = pd.DataFrame({'Actual_TOTAL_LESSONS': y_test, 'Predicted_TOTAL_LESSONS': y_pred})\n",
    "    results_df['DIFFERENCE'] = results_df['Predicted_TOTAL_LESSONS'] - results_df['Actual_TOTAL_LESSONS']\n",
    "    print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa92af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c4a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94c44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5268dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494e03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac4e2707",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82de7c3",
   "metadata": {},
   "source": [
    "Summary of the Code\n",
    "Load the Dataset: The dataset contains information about members of the swim club, including the number of lessons they’ve taken across different classes.\n",
    "Prepare the Data: The data for leavers is extracted. The lesson columns (e.g., PreSchool 1, PreSchool 2, etc.) are selected, and the \"variance to median\" (VTM) for each class is calculated. The VTM represents how much a member deviates from the median number of lessons taken for each class.\n",
    "Binary Target Variable: The total number of lessons (TOTAL QUANTITY OF LESSONS) is converted into a binary target, with members categorized as having taken more than 50 lessons (1) or 50 or fewer lessons (0).\n",
    "Train Logistic Regression: The model is trained on progressively fewer classes, removing three classes at a time. For each iteration:\n",
    "The logistic regression model is trained using the VTM columns and TOTAL QUANTITY OF LESSONS.\n",
    "The dataset is split into training and testing sets.\n",
    "The model is fitted, and predictions are made.\n",
    "Performance metrics like accuracy, classification report, and confusion matrix are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optional if already loaded\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "df = pd.read_excel('swimclass_rawdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae31e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes used: 14 | Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8336\n",
      "           1       1.00      1.00      1.00      4591\n",
      "\n",
      "    accuracy                           1.00     12927\n",
      "   macro avg       1.00      1.00      1.00     12927\n",
      "weighted avg       1.00      1.00      1.00     12927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8336    0]\n",
      " [   0 4591]]\n",
      "       Actual_BINARY_LESSONS  Predicted_BINARY_LESSONS  DIFFERENCE\n",
      "20075                      0                         0           0\n",
      "30546                      1                         1           0\n",
      "20806                      0                         0           0\n",
      "39438                      0                         0           0\n",
      "18229                      1                         1           0\n",
      "Classes used: 11 | Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8336\n",
      "           1       1.00      1.00      1.00      4591\n",
      "\n",
      "    accuracy                           1.00     12927\n",
      "   macro avg       1.00      1.00      1.00     12927\n",
      "weighted avg       1.00      1.00      1.00     12927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8336    0]\n",
      " [   0 4591]]\n",
      "       Actual_BINARY_LESSONS  Predicted_BINARY_LESSONS  DIFFERENCE\n",
      "20075                      0                         0           0\n",
      "30546                      1                         1           0\n",
      "20806                      0                         0           0\n",
      "39438                      0                         0           0\n",
      "18229                      1                         1           0\n",
      "Classes used: 8 | Accuracy: 0.98\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      8336\n",
      "           1       0.98      0.96      0.97      4591\n",
      "\n",
      "    accuracy                           0.98     12927\n",
      "   macro avg       0.98      0.97      0.98     12927\n",
      "weighted avg       0.98      0.98      0.98     12927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8246   90]\n",
      " [ 187 4404]]\n",
      "       Actual_BINARY_LESSONS  Predicted_BINARY_LESSONS  DIFFERENCE\n",
      "20075                      0                         0           0\n",
      "30546                      1                         1           0\n",
      "20806                      0                         0           0\n",
      "39438                      0                         0           0\n",
      "18229                      1                         1           0\n",
      "Classes used: 5 | Accuracy: 0.91\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93      8336\n",
      "           1       0.93      0.81      0.87      4591\n",
      "\n",
      "    accuracy                           0.91     12927\n",
      "   macro avg       0.92      0.89      0.90     12927\n",
      "weighted avg       0.91      0.91      0.91     12927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8046  290]\n",
      " [ 867 3724]]\n",
      "       Actual_BINARY_LESSONS  Predicted_BINARY_LESSONS  DIFFERENCE\n",
      "20075                      0                         0           0\n",
      "30546                      1                         1           0\n",
      "20806                      0                         0           0\n",
      "39438                      0                         0           0\n",
      "18229                      1                         1           0\n",
      "Classes used: 2 | Accuracy: 0.71\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.89      0.80      8336\n",
      "           1       0.65      0.36      0.47      4591\n",
      "\n",
      "    accuracy                           0.71     12927\n",
      "   macro avg       0.69      0.63      0.63     12927\n",
      "weighted avg       0.70      0.71      0.68     12927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7448  888]\n",
      " [2919 1672]]\n",
      "       Actual_BINARY_LESSONS  Predicted_BINARY_LESSONS  DIFFERENCE\n",
      "20075                      0                         0           0\n",
      "30546                      1                         0          -1\n",
      "20806                      0                         0           0\n",
      "39438                      0                         0           0\n",
      "18229                      1                         1           0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data for leavers (ignore current members for now)\n",
    "leavers_df = df[df['Current_member'] == 0].copy()\n",
    "\n",
    "# Step 3: Define the class columns\n",
    "lesson_columns = ['PreSchool 1', 'PreSchool 2', 'PreSchool 3', 'PreSchool 4', 'PreSchool 5',\n",
    "                  'PreSchool 6', 'Academy 1', 'Academy 2', 'Academy 3', 'Academy 4', \n",
    "                  'Academy 5', 'Academy 6', 'BEGINNERS', 'INTERMEDIATE']\n",
    "\n",
    "# Step 4: Calculate Variance to Median (VTM) for each class column\n",
    "for col in lesson_columns:\n",
    "    median_value = leavers_df.loc[leavers_df[col] > 0, col].median()  # Exclude zero values\n",
    "    leavers_df.loc[:, f'{col}_VTM'] = leavers_df[col] - median_value  # Ensure .loc is used properly\n",
    "\n",
    "# Step 5: Create a binary target (whether TOTAL_QUANTITY_OF_LESSONS > 50)\n",
    "leavers_df.loc[:, 'LESSONS_BINARY'] = (leavers_df['TOTAL QUANTITY OF LESSONS'] > 50).astype(int)\n",
    "\n",
    "# Step 6: Train on progressively reduced class data\n",
    "for classes_used in range(len(lesson_columns), 1, -3):  # Remove class data progressively, 3 at a time\n",
    "    selected_classes = lesson_columns[:classes_used]\n",
    "    features = [f'{col}_VTM' for col in selected_classes]  # Exclude TOTAL_QUANTITY_OF_LESSONS from features\n",
    "    \n",
    "    # Step 7: Define the target (Binary: Did they take more than 50 lessons?)\n",
    "    X_leavers = leavers_df[features].copy()\n",
    "    y_leavers = leavers_df['LESSONS_BINARY'].copy()\n",
    "    \n",
    "    # Step 8: Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Step 9: Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 10: Train the logistic regression model\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Step 11: Make predictions and evaluate the model\n",
    "    y_pred = log_reg.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Classes used: {classes_used} | Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # Print detailed classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Optional: Compare actual vs predicted binary lessons\n",
    "    results_df = pd.DataFrame({'Actual_BINARY_LESSONS': y_test, 'Predicted_BINARY_LESSONS': y_pred})\n",
    "    results_df['DIFFERENCE'] = results_df['Predicted_BINARY_LESSONS'] - results_df['Actual_BINARY_LESSONS']\n",
    "    print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e2714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab2bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b43d23e",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c6ef9",
   "metadata": {},
   "source": [
    "The goal of the code is to predict the Total Quantity of Lessons taken by swim club members who have left the club, based on the sequence of classes they took. The code progressively removes class data to test how the model's prediction accuracy changes with less available information.\n",
    "\n",
    "Steps:\n",
    "Loading the Dataset: The dataset is loaded from an Excel file, focusing on members who have left (Current_member == 0).\n",
    "\n",
    "Defining Class Columns: A list of swim class levels (e.g., 'PreSchool 1', 'Academy 1', etc.) is created. These columns represent different swim class levels that members have taken.\n",
    "\n",
    "Calculating Variance to Median (VTM): For each class, the median value is calculated based on the number of lessons taken by the members. Then, the Variance to Median (VTM) is computed for each member, representing how their lessons differ from the median value for that class. This provides a normalized feature to work with for modeling.\n",
    "\n",
    "Training the Model with Progressively Reduced Class Data: The core of the code tests how well the Random Forest model performs when progressively fewer class data points are available:\n",
    "\n",
    "The classes are reduced by 3 in each iteration (starting from all 14 class columns and reducing down).\n",
    "In each iteration, the remaining classes' VTM features are used to train the model.\n",
    "The target variable is Total Quantity of Lessons, which represents the total lessons a member took before leaving.\n",
    "Modeling with Random Forest:\n",
    "\n",
    "The model uses Random Forest regression to predict the total number of lessons based on the available class progression data.\n",
    "The training and testing sets are created using train_test_split from scikit-learn.\n",
    "The features are scaled using StandardScaler for better model performance.\n",
    "The model is then trained, and predictions are made on the test set.\n",
    "Evaluating Model Performance: After each iteration, the model's performance is evaluated using Mean Squared Error (MSE) and R-squared (R²) metrics:\n",
    "\n",
    "MSE measures the average squared difference between the predicted and actual total lessons.\n",
    "R² indicates how well the model fits the data, with 1.00 being a perfect fit.\n",
    "Results for each iteration (with progressively fewer classes) are printed out, including the MSE, R², and the difference between actual and predicted lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54f4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a50eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional Step if not loaded\n",
    "\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "df = pd.read_excel('swimclass_rawdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e51078a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes used: 14 | Mean Squared Error: 39.16 | R-squared: 0.99\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                    39.00        0.00\n",
      "30546                   105                   108.96        3.96\n",
      "20806                    35                    34.98       -0.02\n",
      "39438                    30                    30.00        0.00\n",
      "18229                   134                   130.38       -3.62\n",
      "Classes used: 11 | Mean Squared Error: 40.97 | R-squared: 0.98\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                    39.00        0.00\n",
      "30546                   105                   108.93        3.93\n",
      "20806                    35                    34.98       -0.02\n",
      "39438                    30                    30.00        0.00\n",
      "18229                   134                   130.20       -3.80\n",
      "Classes used: 8 | Mean Squared Error: 239.60 | R-squared: 0.91\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                    39.00        0.00\n",
      "30546                   105                   123.48       18.48\n",
      "20806                    35                    34.95       -0.05\n",
      "39438                    30                    30.00        0.00\n",
      "18229                   134                   130.31       -3.69\n",
      "Classes used: 5 | Mean Squared Error: 784.51 | R-squared: 0.71\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                52.708716   13.708716\n",
      "30546                   105               158.370000   53.370000\n",
      "20806                    35                35.260000    0.260000\n",
      "39438                    30                39.312013    9.312013\n",
      "18229                   134               137.300000    3.300000\n",
      "Classes used: 2 | Mean Squared Error: 2290.42 | R-squared: 0.14\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                47.718586    8.718586\n",
      "30546                   105                46.108937  -58.891063\n",
      "20806                    35                47.718586   12.718586\n",
      "39438                    30                47.718586   17.718586\n",
      "18229                   134               124.067541   -9.932459\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data for leavers (ignore current members for now)\n",
    "leavers_df = df[df['Current_member'] == 0].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Step 3: Define the class columns\n",
    "lesson_columns = ['PreSchool 1', 'PreSchool 2', 'PreSchool 3', 'PreSchool 4', 'PreSchool 5',\n",
    "                  'PreSchool 6', 'Academy 1', 'Academy 2', 'Academy 3', 'Academy 4', \n",
    "                  'Academy 5', 'Academy 6', 'BEGINNERS', 'INTERMEDIATE']\n",
    "\n",
    "# Step 4: Calculate Variance to Median (VTM) for each class column\n",
    "for col in lesson_columns:\n",
    "    median_value = leavers_df.loc[leavers_df[col] > 0, col].median()  # Exclude zero values and use .loc[]\n",
    "    leavers_df.loc[:, f'{col}_VTM'] = leavers_df[col] - median_value  # Explicit .loc[] usage to avoid the warning\n",
    "\n",
    "# Step 5: Train on progressively reduced class data\n",
    "for classes_used in range(len(lesson_columns), 1, -3):  # Remove class data progressively, 3 at a time\n",
    "    selected_classes = lesson_columns[:classes_used]\n",
    "    features = [f'{col}_VTM' for col in selected_classes] + ['TOTAL QUANTITY OF LESSONS']  # Exclude TOTAL_QUANTITY_OF_LESSONS\n",
    "    \n",
    "    # Step 6: Define the target (Total lessons we want to predict)\n",
    "    leavers_df.loc[:, 'PAST_LESSONS'] = leavers_df[lesson_columns].sum(axis=1)  # Known lessons taken so far\n",
    "    \n",
    "    # Remove TOTAL_QUANTITY_OF_LESSONS for training\n",
    "    X_leavers = leavers_df[features].drop(columns=['TOTAL QUANTITY OF LESSONS'])\n",
    "    y_leavers = leavers_df['TOTAL QUANTITY OF LESSONS']  # Total lessons as target\n",
    "    \n",
    "    # Step 7: Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Step 8: Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 9: Train the Random Forest model\n",
    "    rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Step 10: Make predictions and evaluate the model\n",
    "    y_pred = rf_reg.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results for each iteration\n",
    "    print(f\"Classes used: {classes_used} | Mean Squared Error: {mse:.2f} | R-squared: {r2:.2f}\")\n",
    "    \n",
    "    # Compare actual vs predicted total lessons\n",
    "    results_df = pd.DataFrame({'Actual_TOTAL_LESSONS': y_test, 'Predicted_TOTAL_LESSONS': y_pred})\n",
    "    results_df['DIFFERENCE'] = results_df['Predicted_TOTAL_LESSONS'] - results_df['Actual_TOTAL_LESSONS']\n",
    "    print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f17d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4e50b1b",
   "metadata": {},
   "source": [
    "XG boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca1139",
   "metadata": {},
   "source": [
    "This code builds an XGBoost regression model to predict the Total Quantity of Lessons taken by leavers from a swim club based on the classes they have taken. It progressively reduces the number of classes used for training to observe how the model performs with fewer features. \n",
    "\n",
    "Dataset Preparation:\n",
    "\n",
    "The dataset is loaded, and leavers (students who have left the club) are selected.\n",
    "The classes taken by leavers are listed, and a new feature (_VTM or Variance to Median) is created for each class. This represents how much a student's class count differs from the median value for that class.\n",
    "Training and Prediction Process:\n",
    "\n",
    "The model is trained using a progressively reduced number of class features (starting with 14 classes and reducing by 3 at each step).\n",
    "The target variable is the TOTAL QUANTITY OF LESSONS, which the model aims to predict.\n",
    "For each iteration:\n",
    "The data is split into training and testing sets.\n",
    "The features are scaled using StandardScaler.\n",
    "An XGBoost regressor is trained using the current set of features.\n",
    "Predictions are made on the test set.\n",
    "Mean Squared Error (MSE) and R-squared scores are computed to evaluate the model's performance.\n",
    "The actual and predicted lesson totals are compared, and the differences are printed.\n",
    "Performance Monitoring:\n",
    "\n",
    "As the number of features (classes) is reduced, the model's performance is evaluated to see how well it can predict the total number of lessons based on progressively fewer data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcf1b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional Step if not loaded\n",
    "\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "df = pd.read_excel('swimclass_rawdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "141f0ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes used: 14 | Mean Squared Error: 23.69 | R-squared: 0.99\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                39.050774    0.050774\n",
      "30546                   105               121.588165   16.588165\n",
      "20806                    35                32.085907   -2.914093\n",
      "39438                    30                31.703869    1.703869\n",
      "18229                   134               137.799164    3.799164\n",
      "Classes used: 11 | Mean Squared Error: 26.64 | R-squared: 0.99\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                39.783947    0.783947\n",
      "30546                   105               120.549370   15.549370\n",
      "20806                    35                32.642548   -2.357452\n",
      "39438                    30                31.172129    1.172129\n",
      "18229                   134               136.474884    2.474884\n",
      "Classes used: 8 | Mean Squared Error: 224.35 | R-squared: 0.92\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                39.683647    0.683647\n",
      "30546                   105               123.967422   18.967422\n",
      "20806                    35                33.509502   -1.490498\n",
      "39438                    30                32.416870    2.416870\n",
      "18229                   134               134.097260    0.097260\n",
      "Classes used: 5 | Mean Squared Error: 706.84 | R-squared: 0.74\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                50.783604   11.783604\n",
      "30546                   105               124.945930   19.945930\n",
      "20806                    35                59.293854   24.293854\n",
      "39438                    30                38.381546    8.381546\n",
      "18229                   134               185.702209   51.702209\n",
      "Classes used: 2 | Mean Squared Error: 2239.14 | R-squared: 0.16\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                47.681198    8.681198\n",
      "30546                   105                44.389759  -60.610241\n",
      "20806                    35                47.681198   12.681198\n",
      "39438                    30                47.681198   17.681198\n",
      "18229                   134               121.246719  -12.753281\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data for leavers (ignore current members for now)\n",
    "leavers_df = df[df['Current_member'] == 0].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Step 3: Define the class columns\n",
    "lesson_columns = ['PreSchool 1', 'PreSchool 2', 'PreSchool 3', 'PreSchool 4', 'PreSchool 5',\n",
    "                  'PreSchool 6', 'Academy 1', 'Academy 2', 'Academy 3', 'Academy 4', \n",
    "                  'Academy 5', 'Academy 6', 'BEGINNERS', 'INTERMEDIATE']\n",
    "\n",
    "# Step 4: Calculate Variance to Median (VTM) for each class column\n",
    "for col in lesson_columns:\n",
    "    median_value = leavers_df.loc[leavers_df[col] > 0, col].median()  # Exclude zero values\n",
    "    leavers_df.loc[:, f'{col}_VTM'] = leavers_df[col] - median_value\n",
    "\n",
    "# Step 5: Train on progressively reduced class data\n",
    "for classes_used in range(len(lesson_columns), 1, -3):  # Remove class data progressively, 3 at a time\n",
    "    selected_classes = lesson_columns[:classes_used]\n",
    "    features = [f'{col}_VTM' for col in selected_classes] + ['TOTAL QUANTITY OF LESSONS']  # Exclude TOTAL_QUANTITY_OF_LESSONS\n",
    "    \n",
    "    # Step 6: Define the target (Total lessons we want to predict)\n",
    "    leavers_df.loc[:, 'PAST_LESSONS'] = leavers_df[lesson_columns].sum(axis=1)  # Known lessons taken so far\n",
    "    \n",
    "    # Remove TOTAL_QUANTITY_OF_LESSONS for training\n",
    "    X_leavers = leavers_df[features].drop(columns=['TOTAL QUANTITY OF LESSONS'])\n",
    "    y_leavers = leavers_df['TOTAL QUANTITY OF LESSONS']  # Total lessons as target\n",
    "    \n",
    "    # Step 7: Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Step 8: Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 9: Train the XGBoost regression model\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "    xgb_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Step 10: Make predictions and evaluate the model\n",
    "    y_pred = xgb_reg.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results for each iteration\n",
    "    print(f\"Classes used: {classes_used} | Mean Squared Error: {mse:.2f} | R-squared: {r2:.2f}\")\n",
    "    \n",
    "    # Optional: Compare actual vs predicted total lessons\n",
    "    results_df = pd.DataFrame({'Actual_TOTAL_LESSONS': y_test, 'Predicted_TOTAL_LESSONS': y_pred})\n",
    "    results_df['DIFFERENCE'] = results_df['Predicted_TOTAL_LESSONS'] - results_df['Actual_TOTAL_LESSONS']\n",
    "    print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc1037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c15cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b1e5413",
   "metadata": {},
   "source": [
    "ANN approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2ec18",
   "metadata": {},
   "source": [
    "ANN Model Setup:\n",
    "\n",
    "A Sequential model is created with:\n",
    "Input Layer: Accepts the scaled feature set.\n",
    "3 Hidden Layers: With 128, 64, and 32 neurons respectively, each using the ReLU activation function.\n",
    "Output Layer: Predicts the total number of lessons.\n",
    "The model uses the Adam optimizer and minimizes the mean_squared_error loss function.\n",
    "Model Training:\n",
    "\n",
    "The model is trained with 100 epochs, using a batch size of 32. Verbose output is set to show training progress every 10 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa4d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional Step if not loaded\n",
    "\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "df = pd.read_excel('swimclass_rawdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f384ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "755/755 - 4s - 6ms/step - loss: 412.2234 - val_loss: 14.7607\n",
      "Epoch 2/100\n",
      "755/755 - 2s - 3ms/step - loss: 9.0928 - val_loss: 4.6396\n",
      "Epoch 3/100\n",
      "755/755 - 3s - 4ms/step - loss: 3.5924 - val_loss: 6.9199\n",
      "Epoch 4/100\n",
      "755/755 - 1s - 2ms/step - loss: 3.0069 - val_loss: 2.0406\n",
      "Epoch 5/100\n",
      "755/755 - 1s - 2ms/step - loss: 2.4038 - val_loss: 2.2554\n",
      "Epoch 6/100\n",
      "755/755 - 1s - 2ms/step - loss: 2.0165 - val_loss: 2.6981\n",
      "Epoch 7/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.9333 - val_loss: 2.1351\n",
      "Epoch 8/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.3222 - val_loss: 1.0432\n",
      "Epoch 9/100\n",
      "755/755 - 1s - 2ms/step - loss: 2.2088 - val_loss: 0.9423\n",
      "Epoch 10/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.6912 - val_loss: 1.4644\n",
      "Epoch 11/100\n",
      "755/755 - 2s - 3ms/step - loss: 1.4298 - val_loss: 1.2481\n",
      "Epoch 12/100\n",
      "755/755 - 2s - 3ms/step - loss: 1.7476 - val_loss: 1.9115\n",
      "Epoch 13/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.5259 - val_loss: 25.8289\n",
      "Epoch 14/100\n",
      "755/755 - 3s - 3ms/step - loss: 3.1470 - val_loss: 0.9379\n",
      "Epoch 15/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.9483 - val_loss: 0.6855\n",
      "Epoch 16/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.1023 - val_loss: 2.3396\n",
      "Epoch 17/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.3538 - val_loss: 1.3310\n",
      "Epoch 18/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.0330 - val_loss: 0.7972\n",
      "Epoch 19/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.2740 - val_loss: 2.7541\n",
      "Epoch 20/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.2187 - val_loss: 1.3300\n",
      "Epoch 21/100\n",
      "755/755 - 4s - 5ms/step - loss: 1.0932 - val_loss: 1.0205\n",
      "Epoch 22/100\n",
      "755/755 - 2s - 2ms/step - loss: 1.2215 - val_loss: 3.3756\n",
      "Epoch 23/100\n",
      "755/755 - 2s - 3ms/step - loss: 1.2613 - val_loss: 1.8448\n",
      "Epoch 24/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.9725 - val_loss: 1.3481\n",
      "Epoch 25/100\n",
      "755/755 - 3s - 4ms/step - loss: 1.0328 - val_loss: 0.6932\n",
      "Epoch 26/100\n",
      "755/755 - 2s - 3ms/step - loss: 1.3058 - val_loss: 2.5540\n",
      "Epoch 27/100\n",
      "755/755 - 2s - 3ms/step - loss: 0.9141 - val_loss: 0.6805\n",
      "Epoch 28/100\n",
      "755/755 - 3s - 4ms/step - loss: 1.2661 - val_loss: 1.4372\n",
      "Epoch 29/100\n",
      "755/755 - 2s - 2ms/step - loss: 1.1130 - val_loss: 0.5675\n",
      "Epoch 30/100\n",
      "755/755 - 3s - 3ms/step - loss: 1.1200 - val_loss: 0.6553\n",
      "Epoch 31/100\n",
      "755/755 - 2s - 2ms/step - loss: 1.1406 - val_loss: 1.6521\n",
      "Epoch 32/100\n",
      "755/755 - 3s - 3ms/step - loss: 0.8613 - val_loss: 0.6218\n",
      "Epoch 33/100\n",
      "755/755 - 2s - 3ms/step - loss: 1.0438 - val_loss: 0.9547\n",
      "Epoch 34/100\n",
      "755/755 - 2s - 3ms/step - loss: 0.7635 - val_loss: 1.1965\n",
      "Epoch 35/100\n",
      "755/755 - 3s - 4ms/step - loss: 1.0782 - val_loss: 0.7514\n",
      "Epoch 36/100\n",
      "755/755 - 2s - 3ms/step - loss: 0.9146 - val_loss: 0.7715\n",
      "Epoch 37/100\n",
      "755/755 - 2s - 3ms/step - loss: 0.8249 - val_loss: 0.7628\n",
      "Epoch 38/100\n",
      "755/755 - 3s - 3ms/step - loss: 0.7278 - val_loss: 0.6128\n",
      "Epoch 39/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.9651 - val_loss: 2.2000\n",
      "Epoch 40/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.8568 - val_loss: 1.7977\n",
      "Epoch 41/100\n",
      "755/755 - 3s - 4ms/step - loss: 0.9331 - val_loss: 0.6985\n",
      "Epoch 42/100\n",
      "755/755 - 3s - 3ms/step - loss: 0.6939 - val_loss: 0.9619\n",
      "Epoch 43/100\n",
      "755/755 - 2s - 2ms/step - loss: 1.0158 - val_loss: 0.7456\n",
      "Epoch 44/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.9834 - val_loss: 0.5476\n",
      "Epoch 45/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.8240 - val_loss: 0.5402\n",
      "Epoch 46/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.8354 - val_loss: 0.6871\n",
      "Epoch 47/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.7785 - val_loss: 0.5258\n",
      "Epoch 48/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7973 - val_loss: 0.7708\n",
      "Epoch 49/100\n",
      "755/755 - 1s - 2ms/step - loss: 1.1412 - val_loss: 0.8926\n",
      "Epoch 50/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7054 - val_loss: 0.6712\n",
      "Epoch 51/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.8042 - val_loss: 0.5226\n",
      "Epoch 52/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.9283 - val_loss: 0.6158\n",
      "Epoch 53/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7582 - val_loss: 0.5965\n",
      "Epoch 54/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7452 - val_loss: 1.6730\n",
      "Epoch 55/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.8216 - val_loss: 1.1292\n",
      "Epoch 56/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6498 - val_loss: 0.4970\n",
      "Epoch 57/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6882 - val_loss: 0.5589\n",
      "Epoch 58/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7043 - val_loss: 0.6975\n",
      "Epoch 59/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7257 - val_loss: 1.1407\n",
      "Epoch 60/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7108 - val_loss: 0.7241\n",
      "Epoch 61/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6763 - val_loss: 0.7155\n",
      "Epoch 62/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.7672 - val_loss: 1.5881\n",
      "Epoch 63/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.7670 - val_loss: 0.6612\n",
      "Epoch 64/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.5996 - val_loss: 0.5172\n",
      "Epoch 65/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6636 - val_loss: 2.9100\n",
      "Epoch 66/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.8303 - val_loss: 1.1141\n",
      "Epoch 67/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5862 - val_loss: 1.1403\n",
      "Epoch 68/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6487 - val_loss: 0.7550\n",
      "Epoch 69/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.9508 - val_loss: 0.4657\n",
      "Epoch 70/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5533 - val_loss: 1.0352\n",
      "Epoch 71/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5586 - val_loss: 0.5628\n",
      "Epoch 72/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5838 - val_loss: 0.6762\n",
      "Epoch 73/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.8217 - val_loss: 0.6909\n",
      "Epoch 74/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.6883 - val_loss: 1.0601\n",
      "Epoch 75/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.8250 - val_loss: 0.6111\n",
      "Epoch 76/100\n",
      "755/755 - 2s - 3ms/step - loss: 0.5381 - val_loss: 0.6437\n",
      "Epoch 77/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5820 - val_loss: 0.4689\n",
      "Epoch 78/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.6531 - val_loss: 0.6913\n",
      "Epoch 79/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6690 - val_loss: 0.5659\n",
      "Epoch 80/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.4820 - val_loss: 1.1071\n",
      "Epoch 81/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6395 - val_loss: 1.3473\n",
      "Epoch 82/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.7303 - val_loss: 1.0125\n",
      "Epoch 83/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6391 - val_loss: 0.8517\n",
      "Epoch 84/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.6160 - val_loss: 0.6936\n",
      "Epoch 85/100\n",
      "755/755 - 2s - 3ms/step - loss: 0.5239 - val_loss: 0.9582\n",
      "Epoch 86/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5636 - val_loss: 0.5481\n",
      "Epoch 87/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.6107 - val_loss: 0.6104\n",
      "Epoch 88/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5861 - val_loss: 0.4707\n",
      "Epoch 89/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5232 - val_loss: 0.4861\n",
      "Epoch 90/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.5061 - val_loss: 0.6269\n",
      "Epoch 91/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.6256 - val_loss: 0.9277\n",
      "Epoch 92/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.4994 - val_loss: 1.4828\n",
      "Epoch 93/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.8861 - val_loss: 1.3588\n",
      "Epoch 94/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.4244 - val_loss: 0.5405\n",
      "Epoch 95/100\n",
      "755/755 - 2s - 2ms/step - loss: 0.5252 - val_loss: 0.8124\n",
      "Epoch 96/100\n",
      "755/755 - 2s - 3ms/step - loss: 0.4917 - val_loss: 1.0855\n",
      "Epoch 97/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5269 - val_loss: 0.4906\n",
      "Epoch 98/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5973 - val_loss: 1.1877\n",
      "Epoch 99/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.5749 - val_loss: 0.6121\n",
      "Epoch 100/100\n",
      "755/755 - 1s - 2ms/step - loss: 0.4411 - val_loss: 0.5998\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Classes used: 14 | Mean Squared Error: 0.88 | R-squared: 1.00\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                39.270248    0.270248\n",
      "30546                   105               105.376823    0.376823\n",
      "20806                    35                35.337696    0.337696\n",
      "39438                    30                30.255575    0.255575\n",
      "18229                   134               134.399765    0.399765\n",
      "Epoch 1/100\n",
      "755/755 - 4s - 5ms/step - loss: 444.8411 - val_loss: 19.9578\n",
      "Epoch 2/100\n",
      "755/755 - 1s - 2ms/step - loss: 15.5788 - val_loss: 12.8071\n",
      "Epoch 3/100\n",
      "755/755 - 1s - 2ms/step - loss: 12.0100 - val_loss: 13.1209\n",
      "Epoch 4/100\n",
      "755/755 - 2s - 2ms/step - loss: 11.5344 - val_loss: 8.8231\n",
      "Epoch 5/100\n",
      "755/755 - 1s - 2ms/step - loss: 11.6015 - val_loss: 9.4050\n",
      "Epoch 6/100\n",
      "755/755 - 1s - 2ms/step - loss: 11.3560 - val_loss: 10.5190\n",
      "Epoch 7/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.6714 - val_loss: 8.2854\n",
      "Epoch 8/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.4384 - val_loss: 9.6048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.6828 - val_loss: 10.3282\n",
      "Epoch 10/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.7753 - val_loss: 10.3512\n",
      "Epoch 11/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.1942 - val_loss: 12.0977\n",
      "Epoch 12/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.2167 - val_loss: 12.5963\n",
      "Epoch 13/100\n",
      "755/755 - 3s - 3ms/step - loss: 10.8805 - val_loss: 10.0427\n",
      "Epoch 14/100\n",
      "755/755 - 2s - 2ms/step - loss: 9.9687 - val_loss: 9.8668\n",
      "Epoch 15/100\n",
      "755/755 - 2s - 2ms/step - loss: 9.7150 - val_loss: 9.8809\n",
      "Epoch 16/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.3278 - val_loss: 9.0032\n",
      "Epoch 17/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.9755 - val_loss: 8.8046\n",
      "Epoch 18/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.7714 - val_loss: 18.5902\n",
      "Epoch 19/100\n",
      "755/755 - 1s - 2ms/step - loss: 10.2763 - val_loss: 11.2492\n",
      "Epoch 20/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.5668 - val_loss: 8.2163\n",
      "Epoch 21/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.3991 - val_loss: 9.7532\n",
      "Epoch 22/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.3130 - val_loss: 11.0303\n",
      "Epoch 23/100\n",
      "755/755 - 2s - 2ms/step - loss: 10.3206 - val_loss: 10.2671\n",
      "Epoch 24/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.4610 - val_loss: 8.2808\n",
      "Epoch 25/100\n",
      "755/755 - 3s - 4ms/step - loss: 9.3856 - val_loss: 8.0423\n",
      "Epoch 26/100\n",
      "755/755 - 2s - 3ms/step - loss: 9.9838 - val_loss: 8.2887\n",
      "Epoch 27/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.8219 - val_loss: 9.0236\n",
      "Epoch 28/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.2375 - val_loss: 9.5409\n",
      "Epoch 29/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.3268 - val_loss: 11.5074\n",
      "Epoch 30/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.3499 - val_loss: 8.4056\n",
      "Epoch 31/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.0972 - val_loss: 10.7541\n",
      "Epoch 32/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.2560 - val_loss: 10.2386\n",
      "Epoch 33/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.2093 - val_loss: 8.3303\n",
      "Epoch 34/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.1253 - val_loss: 8.5455\n",
      "Epoch 35/100\n",
      "755/755 - 3s - 4ms/step - loss: 8.8619 - val_loss: 7.9977\n",
      "Epoch 36/100\n",
      "755/755 - 2s - 2ms/step - loss: 9.0463 - val_loss: 9.3634\n",
      "Epoch 37/100\n",
      "755/755 - 2s - 2ms/step - loss: 8.9178 - val_loss: 9.0489\n",
      "Epoch 38/100\n",
      "755/755 - 2s - 2ms/step - loss: 9.0299 - val_loss: 9.0470\n",
      "Epoch 39/100\n",
      "755/755 - 2s - 2ms/step - loss: 8.8941 - val_loss: 10.0634\n",
      "Epoch 40/100\n",
      "755/755 - 3s - 4ms/step - loss: 8.9022 - val_loss: 8.7396\n",
      "Epoch 41/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.9397 - val_loss: 9.2672\n",
      "Epoch 42/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.0328 - val_loss: 9.8793\n",
      "Epoch 43/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.9577 - val_loss: 8.3709\n",
      "Epoch 44/100\n",
      "755/755 - 1s - 2ms/step - loss: 9.0216 - val_loss: 8.9007\n",
      "Epoch 45/100\n",
      "755/755 - 2s - 2ms/step - loss: 8.9039 - val_loss: 9.7455\n",
      "Epoch 46/100\n",
      "755/755 - 2s - 2ms/step - loss: 8.4799 - val_loss: 8.2258\n",
      "Epoch 47/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.5099 - val_loss: 14.2260\n",
      "Epoch 48/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.7286 - val_loss: 8.9675\n",
      "Epoch 49/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.9397 - val_loss: 8.3981\n",
      "Epoch 50/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.7896 - val_loss: 8.4674\n",
      "Epoch 51/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.3352 - val_loss: 9.0213\n",
      "Epoch 52/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.5789 - val_loss: 8.2443\n",
      "Epoch 53/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.1778 - val_loss: 16.0721\n",
      "Epoch 54/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.5686 - val_loss: 8.3789\n",
      "Epoch 55/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.2896 - val_loss: 8.6133\n",
      "Epoch 56/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.3849 - val_loss: 8.6045\n",
      "Epoch 57/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.4970 - val_loss: 11.7929\n",
      "Epoch 58/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.3894 - val_loss: 9.1702\n",
      "Epoch 59/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.3701 - val_loss: 9.5268\n",
      "Epoch 60/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.2056 - val_loss: 10.5994\n",
      "Epoch 61/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.0919 - val_loss: 10.2788\n",
      "Epoch 62/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.2899 - val_loss: 11.4551\n",
      "Epoch 63/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.0708 - val_loss: 11.4605\n",
      "Epoch 64/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.2741 - val_loss: 9.4002\n",
      "Epoch 65/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.7916 - val_loss: 14.4156\n",
      "Epoch 66/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.0900 - val_loss: 9.0045\n",
      "Epoch 67/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.8372 - val_loss: 12.7066\n",
      "Epoch 68/100\n",
      "755/755 - 1s - 2ms/step - loss: 8.0265 - val_loss: 10.2464\n",
      "Epoch 69/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.9112 - val_loss: 9.9608\n",
      "Epoch 70/100\n",
      "755/755 - 2s - 3ms/step - loss: 7.8271 - val_loss: 9.2342\n",
      "Epoch 71/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.6408 - val_loss: 10.6763\n",
      "Epoch 72/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.7297 - val_loss: 9.7547\n",
      "Epoch 73/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.4999 - val_loss: 8.8896\n",
      "Epoch 74/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.8364 - val_loss: 10.2756\n",
      "Epoch 75/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.5625 - val_loss: 9.5995\n",
      "Epoch 76/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.8997 - val_loss: 10.5413\n",
      "Epoch 77/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.3789 - val_loss: 9.5830\n",
      "Epoch 78/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.6808 - val_loss: 12.3906\n",
      "Epoch 79/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.5860 - val_loss: 9.5200\n",
      "Epoch 80/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.5806 - val_loss: 9.2097\n",
      "Epoch 81/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.3888 - val_loss: 10.5950\n",
      "Epoch 82/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.5304 - val_loss: 9.0227\n",
      "Epoch 83/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.2898 - val_loss: 13.0016\n",
      "Epoch 84/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.7061 - val_loss: 11.1509\n",
      "Epoch 85/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.3228 - val_loss: 11.4438\n",
      "Epoch 86/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.2319 - val_loss: 9.5052\n",
      "Epoch 87/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.1829 - val_loss: 10.5072\n",
      "Epoch 88/100\n",
      "755/755 - 1s - 2ms/step - loss: 6.9350 - val_loss: 10.0741\n",
      "Epoch 89/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.3551 - val_loss: 12.9057\n",
      "Epoch 90/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.6065 - val_loss: 10.1566\n",
      "Epoch 91/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.3477 - val_loss: 9.1399\n",
      "Epoch 92/100\n",
      "755/755 - 1s - 2ms/step - loss: 6.8423 - val_loss: 10.4767\n",
      "Epoch 93/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.4030 - val_loss: 12.0256\n",
      "Epoch 94/100\n",
      "755/755 - 2s - 2ms/step - loss: 7.0957 - val_loss: 13.5955\n",
      "Epoch 95/100\n",
      "755/755 - 2s - 3ms/step - loss: 7.1001 - val_loss: 10.8961\n",
      "Epoch 96/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.1034 - val_loss: 11.2192\n",
      "Epoch 97/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.2420 - val_loss: 14.3119\n",
      "Epoch 98/100\n",
      "755/755 - 1s - 2ms/step - loss: 6.7861 - val_loss: 11.6931\n",
      "Epoch 99/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.0403 - val_loss: 12.1068\n",
      "Epoch 100/100\n",
      "755/755 - 1s - 2ms/step - loss: 7.1080 - val_loss: 12.0094\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Classes used: 11 | Mean Squared Error: 10.60 | R-squared: 1.00\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                39.504517    0.504517\n",
      "30546                   105               105.519890    0.519890\n",
      "20806                    35                34.829407   -0.170593\n",
      "39438                    30                30.320866    0.320866\n",
      "18229                   134               135.448410    1.448410\n",
      "Epoch 1/100\n",
      "755/755 - 3s - 4ms/step - loss: 670.1428 - val_loss: 279.5083\n",
      "Epoch 2/100\n",
      "755/755 - 1s - 2ms/step - loss: 289.5688 - val_loss: 257.2743\n",
      "Epoch 3/100\n",
      "755/755 - 1s - 2ms/step - loss: 273.1768 - val_loss: 247.7692\n",
      "Epoch 4/100\n",
      "755/755 - 1s - 2ms/step - loss: 266.1669 - val_loss: 242.7168\n",
      "Epoch 5/100\n",
      "755/755 - 1s - 2ms/step - loss: 260.7670 - val_loss: 247.2501\n",
      "Epoch 6/100\n",
      "755/755 - 1s - 2ms/step - loss: 258.7773 - val_loss: 242.2952\n",
      "Epoch 7/100\n",
      "755/755 - 1s - 2ms/step - loss: 258.7186 - val_loss: 236.1550\n",
      "Epoch 8/100\n",
      "755/755 - 1s - 2ms/step - loss: 258.1351 - val_loss: 233.6839\n",
      "Epoch 9/100\n",
      "755/755 - 1s - 2ms/step - loss: 258.0844 - val_loss: 235.9701\n",
      "Epoch 10/100\n",
      "755/755 - 1s - 2ms/step - loss: 254.2547 - val_loss: 254.2635\n",
      "Epoch 11/100\n",
      "755/755 - 1s - 2ms/step - loss: 254.5487 - val_loss: 233.4909\n",
      "Epoch 12/100\n",
      "755/755 - 1s - 2ms/step - loss: 255.5101 - val_loss: 239.7528\n",
      "Epoch 13/100\n",
      "755/755 - 1s - 2ms/step - loss: 254.5039 - val_loss: 235.7259\n",
      "Epoch 14/100\n",
      "755/755 - 1s - 2ms/step - loss: 251.8097 - val_loss: 230.1309\n",
      "Epoch 15/100\n",
      "755/755 - 1s - 2ms/step - loss: 253.1107 - val_loss: 234.4309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "755/755 - 2s - 2ms/step - loss: 251.7899 - val_loss: 234.1118\n",
      "Epoch 17/100\n",
      "755/755 - 2s - 3ms/step - loss: 253.0278 - val_loss: 233.9031\n",
      "Epoch 18/100\n",
      "755/755 - 1s - 2ms/step - loss: 248.9523 - val_loss: 241.4491\n",
      "Epoch 19/100\n",
      "755/755 - 1s - 2ms/step - loss: 252.1931 - val_loss: 233.5018\n",
      "Epoch 20/100\n",
      "755/755 - 1s - 2ms/step - loss: 250.8020 - val_loss: 235.7004\n",
      "Epoch 21/100\n",
      "755/755 - 1s - 2ms/step - loss: 248.8542 - val_loss: 230.9794\n",
      "Epoch 22/100\n",
      "755/755 - 1s - 2ms/step - loss: 248.5922 - val_loss: 227.2683\n",
      "Epoch 23/100\n",
      "755/755 - 1s - 2ms/step - loss: 247.1836 - val_loss: 239.4077\n",
      "Epoch 24/100\n",
      "755/755 - 1s - 2ms/step - loss: 247.7066 - val_loss: 224.1320\n",
      "Epoch 25/100\n",
      "755/755 - 1s - 2ms/step - loss: 247.2053 - val_loss: 226.1307\n",
      "Epoch 26/100\n",
      "755/755 - 1s - 2ms/step - loss: 248.7511 - val_loss: 232.0926\n",
      "Epoch 27/100\n",
      "755/755 - 1s - 2ms/step - loss: 245.1638 - val_loss: 229.2112\n",
      "Epoch 28/100\n",
      "755/755 - 1s - 2ms/step - loss: 244.7131 - val_loss: 231.1870\n",
      "Epoch 29/100\n",
      "755/755 - 2s - 3ms/step - loss: 242.6967 - val_loss: 228.7823\n",
      "Epoch 30/100\n",
      "755/755 - 1s - 2ms/step - loss: 243.5324 - val_loss: 229.5424\n",
      "Epoch 31/100\n",
      "755/755 - 1s - 2ms/step - loss: 245.6552 - val_loss: 238.7344\n",
      "Epoch 32/100\n",
      "755/755 - 1s - 2ms/step - loss: 241.9090 - val_loss: 233.2343\n",
      "Epoch 33/100\n",
      "755/755 - 1s - 2ms/step - loss: 240.8278 - val_loss: 228.2193\n",
      "Epoch 34/100\n",
      "755/755 - 1s - 2ms/step - loss: 240.3656 - val_loss: 225.3636\n",
      "Epoch 35/100\n",
      "755/755 - 1s - 2ms/step - loss: 238.6696 - val_loss: 220.8091\n",
      "Epoch 36/100\n",
      "755/755 - 1s - 2ms/step - loss: 240.5558 - val_loss: 240.2639\n",
      "Epoch 37/100\n",
      "755/755 - 1s - 2ms/step - loss: 241.3726 - val_loss: 220.9845\n",
      "Epoch 38/100\n",
      "755/755 - 2s - 2ms/step - loss: 237.1404 - val_loss: 218.9423\n",
      "Epoch 39/100\n",
      "755/755 - 1s - 2ms/step - loss: 238.2207 - val_loss: 215.7470\n",
      "Epoch 40/100\n",
      "755/755 - 1s - 2ms/step - loss: 236.8106 - val_loss: 219.7842\n",
      "Epoch 41/100\n",
      "755/755 - 2s - 3ms/step - loss: 236.3690 - val_loss: 223.8399\n",
      "Epoch 42/100\n",
      "755/755 - 3s - 3ms/step - loss: 237.3538 - val_loss: 216.0066\n",
      "Epoch 43/100\n",
      "755/755 - 1s - 2ms/step - loss: 236.4807 - val_loss: 228.7718\n",
      "Epoch 44/100\n",
      "755/755 - 1s - 2ms/step - loss: 238.0508 - val_loss: 215.0641\n",
      "Epoch 45/100\n",
      "755/755 - 1s - 2ms/step - loss: 233.3770 - val_loss: 224.2443\n",
      "Epoch 46/100\n",
      "755/755 - 1s - 2ms/step - loss: 232.9838 - val_loss: 219.4773\n",
      "Epoch 47/100\n",
      "755/755 - 1s - 2ms/step - loss: 234.4414 - val_loss: 215.3582\n",
      "Epoch 48/100\n",
      "755/755 - 1s - 2ms/step - loss: 234.1062 - val_loss: 242.7728\n",
      "Epoch 49/100\n",
      "755/755 - 1s - 2ms/step - loss: 234.8262 - val_loss: 221.1520\n",
      "Epoch 50/100\n",
      "755/755 - 1s - 2ms/step - loss: 233.3191 - val_loss: 227.6885\n",
      "Epoch 51/100\n",
      "755/755 - 1s - 2ms/step - loss: 231.9054 - val_loss: 217.7050\n",
      "Epoch 52/100\n",
      "755/755 - 1s - 2ms/step - loss: 232.9194 - val_loss: 210.3915\n",
      "Epoch 53/100\n",
      "755/755 - 1s - 2ms/step - loss: 230.8486 - val_loss: 214.3798\n",
      "Epoch 54/100\n",
      "755/755 - 1s - 2ms/step - loss: 232.2793 - val_loss: 236.0383\n",
      "Epoch 55/100\n",
      "755/755 - 1s - 2ms/step - loss: 230.1107 - val_loss: 218.1310\n",
      "Epoch 56/100\n",
      "755/755 - 1s - 2ms/step - loss: 229.3088 - val_loss: 213.8030\n",
      "Epoch 57/100\n",
      "755/755 - 1s - 2ms/step - loss: 230.4237 - val_loss: 215.2023\n",
      "Epoch 58/100\n",
      "755/755 - 1s - 2ms/step - loss: 229.1296 - val_loss: 213.1401\n",
      "Epoch 59/100\n",
      "755/755 - 1s - 2ms/step - loss: 230.1423 - val_loss: 220.8729\n",
      "Epoch 60/100\n",
      "755/755 - 1s - 2ms/step - loss: 228.5743 - val_loss: 220.7604\n",
      "Epoch 61/100\n",
      "755/755 - 1s - 2ms/step - loss: 227.4373 - val_loss: 220.6131\n",
      "Epoch 62/100\n",
      "755/755 - 1s - 2ms/step - loss: 228.5825 - val_loss: 218.1352\n",
      "Epoch 63/100\n",
      "755/755 - 1s - 2ms/step - loss: 225.5421 - val_loss: 217.0347\n",
      "Epoch 64/100\n",
      "755/755 - 2s - 3ms/step - loss: 229.4607 - val_loss: 221.4991\n",
      "Epoch 65/100\n",
      "755/755 - 1s - 2ms/step - loss: 224.8191 - val_loss: 212.4571\n",
      "Epoch 66/100\n",
      "755/755 - 1s - 2ms/step - loss: 225.0708 - val_loss: 222.8913\n",
      "Epoch 67/100\n",
      "755/755 - 1s - 2ms/step - loss: 225.1124 - val_loss: 214.2764\n",
      "Epoch 68/100\n",
      "755/755 - 1s - 2ms/step - loss: 223.0984 - val_loss: 218.5850\n",
      "Epoch 69/100\n",
      "755/755 - 1s - 2ms/step - loss: 222.9533 - val_loss: 244.3430\n",
      "Epoch 70/100\n",
      "755/755 - 1s - 2ms/step - loss: 225.8464 - val_loss: 218.4941\n",
      "Epoch 71/100\n",
      "755/755 - 1s - 2ms/step - loss: 223.1163 - val_loss: 215.1614\n",
      "Epoch 72/100\n",
      "755/755 - 1s - 2ms/step - loss: 224.1838 - val_loss: 226.7851\n",
      "Epoch 73/100\n",
      "755/755 - 1s - 2ms/step - loss: 222.0277 - val_loss: 216.0622\n",
      "Epoch 74/100\n",
      "755/755 - 1s - 2ms/step - loss: 223.3666 - val_loss: 213.3818\n",
      "Epoch 75/100\n",
      "755/755 - 1s - 2ms/step - loss: 223.9028 - val_loss: 209.9947\n",
      "Epoch 76/100\n",
      "755/755 - 1s - 2ms/step - loss: 225.2177 - val_loss: 210.0507\n",
      "Epoch 77/100\n",
      "755/755 - 1s - 2ms/step - loss: 222.4314 - val_loss: 216.0440\n",
      "Epoch 78/100\n",
      "755/755 - 1s - 2ms/step - loss: 222.2568 - val_loss: 217.4046\n",
      "Epoch 79/100\n",
      "755/755 - 1s - 2ms/step - loss: 220.0854 - val_loss: 212.6281\n",
      "Epoch 80/100\n",
      "755/755 - 1s - 2ms/step - loss: 222.6524 - val_loss: 210.7305\n",
      "Epoch 81/100\n",
      "755/755 - 1s - 2ms/step - loss: 221.8290 - val_loss: 215.2199\n",
      "Epoch 82/100\n",
      "755/755 - 1s - 2ms/step - loss: 220.5113 - val_loss: 212.9107\n",
      "Epoch 83/100\n",
      "755/755 - 1s - 2ms/step - loss: 219.3284 - val_loss: 228.4651\n",
      "Epoch 84/100\n",
      "755/755 - 3s - 3ms/step - loss: 221.9718 - val_loss: 217.6902\n",
      "Epoch 85/100\n",
      "755/755 - 1s - 2ms/step - loss: 221.9682 - val_loss: 213.4798\n",
      "Epoch 86/100\n",
      "755/755 - 1s - 2ms/step - loss: 219.7229 - val_loss: 217.9627\n",
      "Epoch 87/100\n",
      "755/755 - 1s - 2ms/step - loss: 218.9197 - val_loss: 227.0210\n",
      "Epoch 88/100\n",
      "755/755 - 1s - 2ms/step - loss: 220.1079 - val_loss: 222.3388\n",
      "Epoch 89/100\n",
      "755/755 - 1s - 2ms/step - loss: 218.3890 - val_loss: 211.4984\n",
      "Epoch 90/100\n",
      "755/755 - 1s - 2ms/step - loss: 217.0930 - val_loss: 227.2817\n",
      "Epoch 91/100\n",
      "755/755 - 1s - 2ms/step - loss: 218.3943 - val_loss: 211.3387\n",
      "Epoch 92/100\n",
      "755/755 - 1s - 2ms/step - loss: 217.9941 - val_loss: 225.6939\n",
      "Epoch 93/100\n",
      "755/755 - 1s - 2ms/step - loss: 219.1034 - val_loss: 219.7454\n",
      "Epoch 94/100\n",
      "755/755 - 1s - 2ms/step - loss: 217.3122 - val_loss: 229.5625\n",
      "Epoch 95/100\n",
      "755/755 - 1s - 2ms/step - loss: 215.6703 - val_loss: 221.8073\n",
      "Epoch 96/100\n",
      "755/755 - 1s - 2ms/step - loss: 218.1058 - val_loss: 217.7704\n",
      "Epoch 97/100\n",
      "755/755 - 1s - 2ms/step - loss: 217.7598 - val_loss: 210.2856\n",
      "Epoch 98/100\n",
      "755/755 - 1s - 2ms/step - loss: 216.9522 - val_loss: 222.0415\n",
      "Epoch 99/100\n",
      "755/755 - 1s - 2ms/step - loss: 216.1061 - val_loss: 216.1705\n",
      "Epoch 100/100\n",
      "755/755 - 1s - 2ms/step - loss: 215.6550 - val_loss: 218.9815\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Classes used: 8 | Mean Squared Error: 214.65 | R-squared: 0.92\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                36.863304   -2.136696\n",
      "30546                   105               114.073128    9.073128\n",
      "20806                    35                33.848129   -1.151871\n",
      "39438                    30                28.835672   -1.164328\n",
      "18229                   134               129.458054   -4.541946\n",
      "Epoch 1/100\n",
      "755/755 - 3s - 4ms/step - loss: 1219.6860 - val_loss: 861.2018\n",
      "Epoch 2/100\n",
      "755/755 - 1s - 2ms/step - loss: 862.8570 - val_loss: 827.7020\n",
      "Epoch 3/100\n",
      "755/755 - 1s - 2ms/step - loss: 842.2783 - val_loss: 812.2228\n",
      "Epoch 4/100\n",
      "755/755 - 1s - 2ms/step - loss: 825.8673 - val_loss: 794.1077\n",
      "Epoch 5/100\n",
      "755/755 - 1s - 2ms/step - loss: 817.4297 - val_loss: 792.8639\n",
      "Epoch 6/100\n",
      "755/755 - 1s - 2ms/step - loss: 808.3227 - val_loss: 776.7025\n",
      "Epoch 7/100\n",
      "755/755 - 1s - 2ms/step - loss: 802.8303 - val_loss: 809.5838\n",
      "Epoch 8/100\n",
      "755/755 - 1s - 2ms/step - loss: 799.3580 - val_loss: 777.7855\n",
      "Epoch 9/100\n",
      "755/755 - 3s - 3ms/step - loss: 794.4848 - val_loss: 766.5748\n",
      "Epoch 10/100\n",
      "755/755 - 1s - 2ms/step - loss: 791.1229 - val_loss: 785.1477\n",
      "Epoch 11/100\n",
      "755/755 - 1s - 2ms/step - loss: 788.5392 - val_loss: 755.9323\n",
      "Epoch 12/100\n",
      "755/755 - 1s - 2ms/step - loss: 787.6499 - val_loss: 762.8442\n",
      "Epoch 13/100\n",
      "755/755 - 1s - 2ms/step - loss: 781.5828 - val_loss: 757.3082\n",
      "Epoch 14/100\n",
      "755/755 - 1s - 2ms/step - loss: 783.2986 - val_loss: 762.2309\n",
      "Epoch 15/100\n",
      "755/755 - 1s - 2ms/step - loss: 781.3795 - val_loss: 778.3247\n",
      "Epoch 16/100\n",
      "755/755 - 1s - 2ms/step - loss: 778.2698 - val_loss: 767.8722\n",
      "Epoch 17/100\n",
      "755/755 - 1s - 2ms/step - loss: 779.7067 - val_loss: 746.3950\n",
      "Epoch 18/100\n",
      "755/755 - 1s - 2ms/step - loss: 777.2537 - val_loss: 740.6010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "755/755 - 1s - 2ms/step - loss: 773.8607 - val_loss: 761.8495\n",
      "Epoch 20/100\n",
      "755/755 - 1s - 2ms/step - loss: 775.5841 - val_loss: 760.3206\n",
      "Epoch 21/100\n",
      "755/755 - 1s - 2ms/step - loss: 773.5093 - val_loss: 752.1154\n",
      "Epoch 22/100\n",
      "755/755 - 1s - 2ms/step - loss: 773.7682 - val_loss: 755.7438\n",
      "Epoch 23/100\n",
      "755/755 - 1s - 2ms/step - loss: 772.5822 - val_loss: 741.0767\n",
      "Epoch 24/100\n",
      "755/755 - 1s - 2ms/step - loss: 772.0166 - val_loss: 758.3414\n",
      "Epoch 25/100\n",
      "755/755 - 2s - 3ms/step - loss: 771.9504 - val_loss: 740.2029\n",
      "Epoch 26/100\n",
      "755/755 - 1s - 2ms/step - loss: 769.0836 - val_loss: 744.7178\n",
      "Epoch 27/100\n",
      "755/755 - 1s - 2ms/step - loss: 771.5261 - val_loss: 744.9844\n",
      "Epoch 28/100\n",
      "755/755 - 2s - 2ms/step - loss: 771.3478 - val_loss: 738.9144\n",
      "Epoch 29/100\n",
      "755/755 - 1s - 2ms/step - loss: 769.9266 - val_loss: 739.8315\n",
      "Epoch 30/100\n",
      "755/755 - 1s - 2ms/step - loss: 769.8257 - val_loss: 748.9245\n",
      "Epoch 31/100\n",
      "755/755 - 2s - 2ms/step - loss: 767.6950 - val_loss: 759.1513\n",
      "Epoch 32/100\n",
      "755/755 - 1s - 2ms/step - loss: 773.3635 - val_loss: 755.1484\n",
      "Epoch 33/100\n",
      "755/755 - 1s - 2ms/step - loss: 767.0029 - val_loss: 739.4907\n",
      "Epoch 34/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.1677 - val_loss: 748.0426\n",
      "Epoch 35/100\n",
      "755/755 - 1s - 2ms/step - loss: 768.1239 - val_loss: 765.4130\n",
      "Epoch 36/100\n",
      "755/755 - 1s - 2ms/step - loss: 768.3578 - val_loss: 745.3469\n",
      "Epoch 37/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.0709 - val_loss: 749.8240\n",
      "Epoch 38/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.1361 - val_loss: 744.1190\n",
      "Epoch 39/100\n",
      "755/755 - 1s - 2ms/step - loss: 765.9265 - val_loss: 746.4745\n",
      "Epoch 40/100\n",
      "755/755 - 1s - 2ms/step - loss: 769.4012 - val_loss: 734.6149\n",
      "Epoch 41/100\n",
      "755/755 - 1s - 2ms/step - loss: 767.5631 - val_loss: 741.4113\n",
      "Epoch 42/100\n",
      "755/755 - 1s - 2ms/step - loss: 768.9722 - val_loss: 731.0248\n",
      "Epoch 43/100\n",
      "755/755 - 1s - 2ms/step - loss: 767.4197 - val_loss: 737.8088\n",
      "Epoch 44/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.2579 - val_loss: 733.8901\n",
      "Epoch 45/100\n",
      "755/755 - 1s - 2ms/step - loss: 763.9088 - val_loss: 740.1481\n",
      "Epoch 46/100\n",
      "755/755 - 1s - 2ms/step - loss: 767.2260 - val_loss: 792.4370\n",
      "Epoch 47/100\n",
      "755/755 - 1s - 2ms/step - loss: 764.1993 - val_loss: 749.9258\n",
      "Epoch 48/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.1113 - val_loss: 772.4702\n",
      "Epoch 49/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.5083 - val_loss: 748.6977\n",
      "Epoch 50/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.6935 - val_loss: 736.3673\n",
      "Epoch 51/100\n",
      "755/755 - 1s - 2ms/step - loss: 763.4514 - val_loss: 737.0445\n",
      "Epoch 52/100\n",
      "755/755 - 1s - 2ms/step - loss: 763.6888 - val_loss: 747.3210\n",
      "Epoch 53/100\n",
      "755/755 - 1s - 2ms/step - loss: 764.7553 - val_loss: 744.0649\n",
      "Epoch 54/100\n",
      "755/755 - 1s - 2ms/step - loss: 766.7013 - val_loss: 749.6045\n",
      "Epoch 55/100\n",
      "755/755 - 1s - 2ms/step - loss: 763.6957 - val_loss: 731.7673\n",
      "Epoch 56/100\n",
      "755/755 - 1s - 2ms/step - loss: 764.6476 - val_loss: 740.4901\n",
      "Epoch 57/100\n",
      "755/755 - 1s - 2ms/step - loss: 763.2151 - val_loss: 730.4328\n",
      "Epoch 58/100\n",
      "755/755 - 2s - 3ms/step - loss: 766.5312 - val_loss: 776.0179\n",
      "Epoch 59/100\n",
      "755/755 - 1s - 2ms/step - loss: 762.1799 - val_loss: 741.0399\n",
      "Epoch 60/100\n",
      "755/755 - 1s - 2ms/step - loss: 761.5828 - val_loss: 737.4530\n",
      "Epoch 61/100\n",
      "755/755 - 1s - 2ms/step - loss: 765.4279 - val_loss: 736.3435\n",
      "Epoch 62/100\n",
      "755/755 - 1s - 2ms/step - loss: 762.8022 - val_loss: 736.8697\n",
      "Epoch 63/100\n",
      "755/755 - 1s - 2ms/step - loss: 762.7199 - val_loss: 742.9236\n",
      "Epoch 64/100\n",
      "755/755 - 1s - 2ms/step - loss: 763.0314 - val_loss: 732.5606\n",
      "Epoch 65/100\n",
      "755/755 - 1s - 2ms/step - loss: 765.3138 - val_loss: 735.9767\n",
      "Epoch 66/100\n",
      "755/755 - 1s - 2ms/step - loss: 761.6175 - val_loss: 732.3713\n",
      "Epoch 67/100\n",
      "755/755 - 1s - 2ms/step - loss: 760.6851 - val_loss: 747.8141\n",
      "Epoch 68/100\n",
      "755/755 - 2s - 2ms/step - loss: 760.1570 - val_loss: 740.4266\n",
      "Epoch 69/100\n",
      "755/755 - 1s - 2ms/step - loss: 762.3846 - val_loss: 733.1227\n",
      "Epoch 70/100\n",
      "755/755 - 3s - 3ms/step - loss: 761.9542 - val_loss: 730.3927\n",
      "Epoch 71/100\n",
      "755/755 - 1s - 2ms/step - loss: 760.6997 - val_loss: 737.0222\n",
      "Epoch 72/100\n",
      "755/755 - 1s - 2ms/step - loss: 761.6688 - val_loss: 741.0391\n",
      "Epoch 73/100\n",
      "755/755 - 1s - 2ms/step - loss: 759.4747 - val_loss: 743.7272\n",
      "Epoch 74/100\n",
      "755/755 - 1s - 2ms/step - loss: 759.7890 - val_loss: 767.8737\n",
      "Epoch 75/100\n",
      "755/755 - 1s - 2ms/step - loss: 758.9507 - val_loss: 739.1913\n",
      "Epoch 76/100\n",
      "755/755 - 1s - 2ms/step - loss: 759.5856 - val_loss: 729.1705\n",
      "Epoch 77/100\n",
      "755/755 - 1s - 2ms/step - loss: 758.3431 - val_loss: 744.9970\n",
      "Epoch 78/100\n",
      "755/755 - 1s - 2ms/step - loss: 758.6144 - val_loss: 729.4425\n",
      "Epoch 79/100\n",
      "755/755 - 1s - 2ms/step - loss: 757.4879 - val_loss: 757.2039\n",
      "Epoch 80/100\n",
      "755/755 - 1s - 2ms/step - loss: 757.9939 - val_loss: 731.4280\n",
      "Epoch 81/100\n",
      "755/755 - 1s - 2ms/step - loss: 759.4624 - val_loss: 737.1117\n",
      "Epoch 82/100\n",
      "755/755 - 2s - 3ms/step - loss: 759.5807 - val_loss: 729.9118\n",
      "Epoch 83/100\n",
      "755/755 - 1s - 2ms/step - loss: 755.1709 - val_loss: 745.9890\n",
      "Epoch 84/100\n",
      "755/755 - 1s - 2ms/step - loss: 755.9236 - val_loss: 738.4087\n",
      "Epoch 85/100\n",
      "755/755 - 1s - 2ms/step - loss: 754.3239 - val_loss: 735.8832\n",
      "Epoch 86/100\n",
      "755/755 - 1s - 2ms/step - loss: 755.5448 - val_loss: 739.7906\n",
      "Epoch 87/100\n",
      "755/755 - 1s - 2ms/step - loss: 756.8763 - val_loss: 731.9505\n",
      "Epoch 88/100\n",
      "755/755 - 1s - 2ms/step - loss: 755.8531 - val_loss: 734.6498\n",
      "Epoch 89/100\n",
      "755/755 - 1s - 2ms/step - loss: 757.9037 - val_loss: 738.4125\n",
      "Epoch 90/100\n",
      "755/755 - 1s - 2ms/step - loss: 757.4485 - val_loss: 736.8724\n",
      "Epoch 91/100\n",
      "755/755 - 1s - 2ms/step - loss: 755.6638 - val_loss: 745.2142\n",
      "Epoch 92/100\n",
      "755/755 - 1s - 2ms/step - loss: 755.4047 - val_loss: 725.5989\n",
      "Epoch 93/100\n",
      "755/755 - 2s - 3ms/step - loss: 759.3607 - val_loss: 741.8810\n",
      "Epoch 94/100\n",
      "755/755 - 1s - 2ms/step - loss: 755.6818 - val_loss: 754.0320\n",
      "Epoch 95/100\n",
      "755/755 - 1s - 2ms/step - loss: 756.6456 - val_loss: 740.2919\n",
      "Epoch 96/100\n",
      "755/755 - 1s - 2ms/step - loss: 754.3317 - val_loss: 739.0692\n",
      "Epoch 97/100\n",
      "755/755 - 1s - 2ms/step - loss: 756.0618 - val_loss: 748.8434\n",
      "Epoch 98/100\n",
      "755/755 - 1s - 2ms/step - loss: 756.1843 - val_loss: 736.1055\n",
      "Epoch 99/100\n",
      "755/755 - 1s - 2ms/step - loss: 754.8713 - val_loss: 727.0477\n",
      "Epoch 100/100\n",
      "755/755 - 1s - 2ms/step - loss: 753.4816 - val_loss: 736.0869\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Classes used: 5 | Mean Squared Error: 700.22 | R-squared: 0.74\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                48.857971    9.857971\n",
      "30546                   105               118.195877   13.195877\n",
      "20806                    35                56.994919   21.994919\n",
      "39438                    30                37.618988    7.618988\n",
      "18229                   134               140.749390    6.749390\n",
      "Epoch 1/100\n",
      "755/755 - 4s - 5ms/step - loss: 2586.6726 - val_loss: 2284.1887\n",
      "Epoch 2/100\n",
      "755/755 - 1s - 2ms/step - loss: 2322.1975 - val_loss: 2281.8542\n",
      "Epoch 3/100\n",
      "755/755 - 1s - 2ms/step - loss: 2319.3762 - val_loss: 2282.3569\n",
      "Epoch 4/100\n",
      "755/755 - 1s - 2ms/step - loss: 2316.8474 - val_loss: 2283.1272\n",
      "Epoch 5/100\n",
      "755/755 - 1s - 2ms/step - loss: 2317.8562 - val_loss: 2274.5706\n",
      "Epoch 6/100\n",
      "755/755 - 1s - 2ms/step - loss: 2314.4268 - val_loss: 2274.2341\n",
      "Epoch 7/100\n",
      "755/755 - 1s - 2ms/step - loss: 2312.1433 - val_loss: 2284.4717\n",
      "Epoch 8/100\n",
      "755/755 - 1s - 2ms/step - loss: 2313.3647 - val_loss: 2301.5803\n",
      "Epoch 9/100\n",
      "755/755 - 1s - 2ms/step - loss: 2312.4712 - val_loss: 2288.5232\n",
      "Epoch 10/100\n",
      "755/755 - 1s - 2ms/step - loss: 2310.7637 - val_loss: 2276.7068\n",
      "Epoch 11/100\n",
      "755/755 - 1s - 2ms/step - loss: 2309.5403 - val_loss: 2270.6272\n",
      "Epoch 12/100\n",
      "755/755 - 1s - 2ms/step - loss: 2308.8271 - val_loss: 2272.5212\n",
      "Epoch 13/100\n",
      "755/755 - 2s - 2ms/step - loss: 2308.4546 - val_loss: 2274.5220\n",
      "Epoch 14/100\n",
      "755/755 - 2s - 2ms/step - loss: 2308.7751 - val_loss: 2273.1057\n",
      "Epoch 15/100\n",
      "755/755 - 1s - 2ms/step - loss: 2309.0430 - val_loss: 2267.1448\n",
      "Epoch 16/100\n",
      "755/755 - 1s - 2ms/step - loss: 2307.8979 - val_loss: 2281.4092\n",
      "Epoch 17/100\n",
      "755/755 - 1s - 2ms/step - loss: 2307.6895 - val_loss: 2266.8264\n",
      "Epoch 18/100\n",
      "755/755 - 1s - 2ms/step - loss: 2306.4055 - val_loss: 2265.0403\n",
      "Epoch 19/100\n",
      "755/755 - 1s - 2ms/step - loss: 2307.2622 - val_loss: 2269.7539\n",
      "Epoch 20/100\n",
      "755/755 - 1s - 2ms/step - loss: 2308.8735 - val_loss: 2275.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "755/755 - 1s - 2ms/step - loss: 2307.9954 - val_loss: 2269.8230\n",
      "Epoch 22/100\n",
      "755/755 - 1s - 2ms/step - loss: 2310.1643 - val_loss: 2265.6365\n",
      "Epoch 23/100\n",
      "755/755 - 1s - 2ms/step - loss: 2308.0464 - val_loss: 2274.1318\n",
      "Epoch 24/100\n",
      "755/755 - 1s - 2ms/step - loss: 2307.1418 - val_loss: 2265.8433\n",
      "Epoch 25/100\n",
      "755/755 - 1s - 2ms/step - loss: 2306.5825 - val_loss: 2271.5562\n",
      "Epoch 26/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.6079 - val_loss: 2267.4973\n",
      "Epoch 27/100\n",
      "755/755 - 1s - 2ms/step - loss: 2309.3877 - val_loss: 2265.2998\n",
      "Epoch 28/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.8220 - val_loss: 2267.6597\n",
      "Epoch 29/100\n",
      "755/755 - 1s - 2ms/step - loss: 2306.6101 - val_loss: 2271.0466\n",
      "Epoch 30/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.9790 - val_loss: 2266.2671\n",
      "Epoch 31/100\n",
      "755/755 - 1s - 2ms/step - loss: 2306.8665 - val_loss: 2268.6763\n",
      "Epoch 32/100\n",
      "755/755 - 1s - 2ms/step - loss: 2306.8708 - val_loss: 2275.8232\n",
      "Epoch 33/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.4404 - val_loss: 2269.3584\n",
      "Epoch 34/100\n",
      "755/755 - 1s - 2ms/step - loss: 2306.0923 - val_loss: 2266.2466\n",
      "Epoch 35/100\n",
      "755/755 - 1s - 2ms/step - loss: 2307.1074 - val_loss: 2267.4543\n",
      "Epoch 36/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.5930 - val_loss: 2265.7773\n",
      "Epoch 37/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.8127 - val_loss: 2280.3982\n",
      "Epoch 38/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.3293 - val_loss: 2277.5325\n",
      "Epoch 39/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.1162 - val_loss: 2278.7983\n",
      "Epoch 40/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.4397 - val_loss: 2271.3564\n",
      "Epoch 41/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.4871 - val_loss: 2262.8035\n",
      "Epoch 42/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.8342 - val_loss: 2269.1511\n",
      "Epoch 43/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.5920 - val_loss: 2274.3792\n",
      "Epoch 44/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.5476 - val_loss: 2263.5713\n",
      "Epoch 45/100\n",
      "755/755 - 1s - 2ms/step - loss: 2307.1389 - val_loss: 2268.2690\n",
      "Epoch 46/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.9795 - val_loss: 2267.0847\n",
      "Epoch 47/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.5457 - val_loss: 2277.4792\n",
      "Epoch 48/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.6091 - val_loss: 2269.1909\n",
      "Epoch 49/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.2427 - val_loss: 2261.9258\n",
      "Epoch 50/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.2705 - val_loss: 2263.8899\n",
      "Epoch 51/100\n",
      "755/755 - 1s - 2ms/step - loss: 2305.3010 - val_loss: 2285.2712\n",
      "Epoch 52/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.5100 - val_loss: 2281.2539\n",
      "Epoch 53/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.3118 - val_loss: 2268.7439\n",
      "Epoch 54/100\n",
      "755/755 - 1s - 2ms/step - loss: 2306.8149 - val_loss: 2265.4563\n",
      "Epoch 55/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.7708 - val_loss: 2268.5288\n",
      "Epoch 56/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.9792 - val_loss: 2272.5554\n",
      "Epoch 57/100\n",
      "755/755 - 1s - 2ms/step - loss: 2301.6294 - val_loss: 2261.6084\n",
      "Epoch 58/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.9355 - val_loss: 2307.7737\n",
      "Epoch 59/100\n",
      "755/755 - 1s - 2ms/step - loss: 2304.7441 - val_loss: 2280.4976\n",
      "Epoch 60/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.3367 - val_loss: 2267.2712\n",
      "Epoch 61/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.1873 - val_loss: 2260.2087\n",
      "Epoch 62/100\n",
      "755/755 - 2s - 2ms/step - loss: 2303.1111 - val_loss: 2263.5620\n",
      "Epoch 63/100\n",
      "755/755 - 2s - 2ms/step - loss: 2300.7446 - val_loss: 2272.3105\n",
      "Epoch 64/100\n",
      "755/755 - 2s - 3ms/step - loss: 2302.9700 - val_loss: 2261.5593\n",
      "Epoch 65/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.7173 - val_loss: 2264.5576\n",
      "Epoch 66/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.0698 - val_loss: 2271.1409\n",
      "Epoch 67/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.3540 - val_loss: 2262.9165\n",
      "Epoch 68/100\n",
      "755/755 - 1s - 2ms/step - loss: 2301.2322 - val_loss: 2266.1616\n",
      "Epoch 69/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.4592 - val_loss: 2274.3013\n",
      "Epoch 70/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.5471 - val_loss: 2260.6902\n",
      "Epoch 71/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.6384 - val_loss: 2267.3755\n",
      "Epoch 72/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.9680 - val_loss: 2265.1438\n",
      "Epoch 73/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.3247 - val_loss: 2264.0845\n",
      "Epoch 74/100\n",
      "755/755 - 2s - 2ms/step - loss: 2303.6631 - val_loss: 2262.8577\n",
      "Epoch 75/100\n",
      "755/755 - 2s - 3ms/step - loss: 2301.5242 - val_loss: 2262.1672\n",
      "Epoch 76/100\n",
      "755/755 - 2s - 2ms/step - loss: 2299.5417 - val_loss: 2272.8123\n",
      "Epoch 77/100\n",
      "755/755 - 1s - 2ms/step - loss: 2300.8186 - val_loss: 2260.3855\n",
      "Epoch 78/100\n",
      "755/755 - 2s - 2ms/step - loss: 2301.9082 - val_loss: 2262.2568\n",
      "Epoch 79/100\n",
      "755/755 - 2s - 2ms/step - loss: 2301.0532 - val_loss: 2268.6130\n",
      "Epoch 80/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.5515 - val_loss: 2261.0620\n",
      "Epoch 81/100\n",
      "755/755 - 1s - 2ms/step - loss: 2300.4817 - val_loss: 2269.5459\n",
      "Epoch 82/100\n",
      "755/755 - 1s - 2ms/step - loss: 2301.6052 - val_loss: 2267.5657\n",
      "Epoch 83/100\n",
      "755/755 - 3s - 4ms/step - loss: 2302.1030 - val_loss: 2261.6951\n",
      "Epoch 84/100\n",
      "755/755 - 2s - 2ms/step - loss: 2304.2944 - val_loss: 2265.4338\n",
      "Epoch 85/100\n",
      "755/755 - 1s - 2ms/step - loss: 2302.5857 - val_loss: 2261.2422\n",
      "Epoch 86/100\n",
      "755/755 - 1s - 2ms/step - loss: 2301.2778 - val_loss: 2260.7432\n",
      "Epoch 87/100\n",
      "755/755 - 1s - 2ms/step - loss: 2298.6572 - val_loss: 2264.7363\n",
      "Epoch 88/100\n",
      "755/755 - 1s - 2ms/step - loss: 2301.4905 - val_loss: 2261.8042\n",
      "Epoch 89/100\n",
      "755/755 - 1s - 2ms/step - loss: 2298.4700 - val_loss: 2268.1567\n",
      "Epoch 90/100\n",
      "755/755 - 1s - 2ms/step - loss: 2299.6223 - val_loss: 2260.5396\n",
      "Epoch 91/100\n",
      "755/755 - 1s - 2ms/step - loss: 2298.6538 - val_loss: 2263.0835\n",
      "Epoch 92/100\n",
      "755/755 - 1s - 2ms/step - loss: 2300.8525 - val_loss: 2266.2466\n",
      "Epoch 93/100\n",
      "755/755 - 1s - 2ms/step - loss: 2303.0933 - val_loss: 2263.9534\n",
      "Epoch 94/100\n",
      "755/755 - 1s - 2ms/step - loss: 2301.5410 - val_loss: 2264.2136\n",
      "Epoch 95/100\n",
      "755/755 - 1s - 2ms/step - loss: 2300.9365 - val_loss: 2260.7493\n",
      "Epoch 96/100\n",
      "755/755 - 2s - 2ms/step - loss: 2300.5801 - val_loss: 2261.4226\n",
      "Epoch 97/100\n",
      "755/755 - 1s - 2ms/step - loss: 2300.2275 - val_loss: 2259.5586\n",
      "Epoch 98/100\n",
      "755/755 - 1s - 2ms/step - loss: 2301.1660 - val_loss: 2262.2576\n",
      "Epoch 99/100\n",
      "755/755 - 1s - 2ms/step - loss: 2300.6709 - val_loss: 2260.4922\n",
      "Epoch 100/100\n",
      "755/755 - 1s - 2ms/step - loss: 2299.6240 - val_loss: 2260.7571\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Classes used: 2 | Mean Squared Error: 2245.45 | R-squared: 0.16\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                45.878181    6.878181\n",
      "30546                   105                40.134293  -64.865707\n",
      "20806                    35                45.878181   10.878181\n",
      "39438                    30                45.878181   15.878181\n",
      "18229                   134               100.031418  -33.968582\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data for leavers (ignore current members for now)\n",
    "leavers_df = df[df['Current_member'] == 0].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Step 3: Define the class columns\n",
    "lesson_columns = ['PreSchool 1', 'PreSchool 2', 'PreSchool 3', 'PreSchool 4', 'PreSchool 5',\n",
    "                  'PreSchool 6', 'Academy 1', 'Academy 2', 'Academy 3', 'Academy 4', \n",
    "                  'Academy 5', 'Academy 6', 'BEGINNERS', 'INTERMEDIATE']\n",
    "\n",
    "# Step 4: Calculate Variance to Median (VTM) for each class column\n",
    "for col in lesson_columns:\n",
    "    median_value = leavers_df.loc[leavers_df[col] > 0, col].median()  # Exclude zero values\n",
    "    leavers_df.loc[:, f'{col}_VTM'] = leavers_df[col] - median_value  # Proper use of .loc to avoid warnings\n",
    "\n",
    "# Step 5: Train on progressively reduced class data\n",
    "for classes_used in range(len(lesson_columns), 1, -3):  # Remove class data progressively, 3 at a time\n",
    "    selected_classes = lesson_columns[:classes_used]\n",
    "    features = [f'{col}_VTM' for col in selected_classes] + ['TOTAL QUANTITY OF LESSONS']  # Exclude TOTAL_QUANTITY_OF_LESSONS\n",
    "    \n",
    "    # Step 6: Define the target (Total lessons we want to predict)\n",
    "    leavers_df['PAST_LESSONS'] = leavers_df[lesson_columns].sum(axis=1)  # Known lessons taken so far\n",
    "    \n",
    "    # Remove TOTAL_QUANTITY_OF_LESSONS for training\n",
    "    X_leavers = leavers_df[features].drop(columns=['TOTAL QUANTITY OF LESSONS'])\n",
    "    y_leavers = leavers_df['TOTAL QUANTITY OF LESSONS']  # Total lessons as target\n",
    "    \n",
    "    # Step 7: Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Step 8: Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 9: Build and train the ANN model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_scaled.shape[1],)))  # Use Input(shape) for the first layer\n",
    "    model.add(Dense(128, activation='relu'))  # 1st hidden layer\n",
    "    model.add(Dense(64, activation='relu'))   # 2nd hidden layer\n",
    "    model.add(Dense(32, activation='relu'))   # 3rd hidden layer\n",
    "    model.add(Dense(1))  # Output layer for regression (single target)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model, showing output every 10 epochs\n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.2)\n",
    "    \n",
    "    # Step 10: Make predictions and evaluate the model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results for each iteration\n",
    "    print(f\"Classes used: {classes_used} | Mean Squared Error: {mse:.2f} | R-squared: {r2:.2f}\")\n",
    "    \n",
    "    # Optional: Compare actual vs predicted total lessons\n",
    "    results_df = pd.DataFrame({'Actual_TOTAL_LESSONS': y_test, 'Predicted_TOTAL_LESSONS': y_pred.flatten()})\n",
    "    results_df['DIFFERENCE'] = results_df['Predicted_TOTAL_LESSONS'] - results_df['Actual_TOTAL_LESSONS']\n",
    "    print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf3eba",
   "metadata": {},
   "source": [
    "Results summary from ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd7e78",
   "metadata": {},
   "source": [
    "Classes used: 14 | Mean Squared Error: 0.88 | R-squared: 1.00\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                39.270248    0.270248\n",
    "30546                   105               105.376823    0.376823\n",
    "20806                    35                35.337696    0.337696\n",
    "39438                    30                30.255575    0.255575\n",
    "18229                   134               134.399765    0.399765\n",
    "\n",
    "Classes used: 11 | Mean Squared Error: 10.60 | R-squared: 1.00\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                39.504517    0.504517\n",
    "30546                   105               105.519890    0.519890\n",
    "20806                    35                34.829407   -0.170593\n",
    "39438                    30                30.320866    0.320866\n",
    "18229                   134               135.448410    1.448410\n",
    "\n",
    "Classes used: 8 | Mean Squared Error: 214.65 | R-squared: 0.92\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                36.863304   -2.136696\n",
    "30546                   105               114.073128    9.073128\n",
    "20806                    35                33.848129   -1.151871\n",
    "39438                    30                28.835672   -1.164328\n",
    "18229                   134               129.458054   -4.541946\n",
    "\n",
    "Classes used: 5 | Mean Squared Error: 700.22 | R-squared: 0.74\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                48.857971    9.857971\n",
    "30546                   105               118.195877   13.195877\n",
    "20806                    35                56.994919   21.994919\n",
    "39438                    30                37.618988    7.618988\n",
    "18229                   134               140.749390    6.749390\n",
    "\n",
    "Classes used: 2 | Mean Squared Error: 2245.45 | R-squared: 0.16\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                45.878181    6.878181\n",
    "30546                   105                40.134293  -64.865707\n",
    "20806                    35                45.878181   10.878181\n",
    "39438                    30                45.878181   15.878181\n",
    "18229                   134               100.031418  -33.968582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a1ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557454fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02aaea5a",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "907aa10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\mbiel\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
      "   ---------------------------------------- 0.0/362.8 kB ? eta -:--:--\n",
      "   ------------------------------------ -- 337.9/362.8 kB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 362.8/362.8 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/233.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 233.0/233.0 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.6/78.6 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbdca77",
   "metadata": {},
   "source": [
    "This code below performs hyperparameter tuning for a neural network model using Optuna, a framework for hyperparameter optimization.Summary of the key steps and components:\n",
    "\n",
    "Libraries:\n",
    "Optuna: Used for optimizing hyperparameters.\n",
    "TensorFlow/Keras: To define and train the neural network model.\n",
    "Scikit-learn: For data splitting, scaling, and calculating performance metrics.\n",
    "Steps Breakdown:\n",
    "Data Preparation:\n",
    "\n",
    "The dataset X_leavers (features) and y_leavers (target) is split into training and testing sets using train_test_split().\n",
    "The features are scaled using StandardScaler to standardize the data for better neural network performance.\n",
    "Objective Function:\n",
    "\n",
    "An objective function is defined for Optuna to optimize. It contains:\n",
    "Hyperparameters to be tuned:\n",
    "optimizer: Chooses between 'adam' and 'rmsprop'.\n",
    "neurons: Specifies the number of neurons in the hidden layer (between 32 and 128).\n",
    "batch_size: Specifies the batch size for training (between 16 and 64).\n",
    "epochs: Specifies the number of training epochs (between 10 and 100).\n",
    "Inside the objective function:\n",
    "A simple feed-forward neural network (ANN) is created using Keras:\n",
    "One hidden layer with a variable number of neurons and a relu activation.\n",
    "A single output layer for regression.\n",
    "The model is compiled with a loss function of mean_squared_error and the selected optimizer.\n",
    "The model is trained using the specified hyperparameters (epochs and batch_size).\n",
    "The Mean Squared Error (MSE) is calculated on the test data and returned as the metric to be minimized by Optuna.\n",
    "Optimization with Optuna:\n",
    "\n",
    "A study is created using optuna.create_study() with the goal to minimize the MSE.\n",
    "The optimisation runs for 20 trials, exploring different combinations of the hyperparameters defined in the objective function.\n",
    "Best Hyperparameters:\n",
    "\n",
    "Once the study completes, the best combination of hyperparameters (optimizer, neurons, batch_size, epochs) and the best MSE are printed.\n",
    "Purpose:\n",
    "The goal is to find the best hyperparameters for the neural network model (optimizer, number of neurons, batch size, and epochs) that minimize the MSE on the test data, ensuring the best possible performance for predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a0bbf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 08:55:59,202] A new study created in memory with name: no-name-f69b08a1-f504-49fe-9f0d-d4b11fc0c1f1\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 08:56:40,738] Trial 0 finished with value: 2255.6711421187574 and parameters: {'optimizer': 'rmsprop', 'neurons': 94, 'batch_size': 34, 'epochs': 40}. Best is trial 0 with value: 2255.6711421187574.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 08:56:59,942] Trial 1 finished with value: 2263.8592430864646 and parameters: {'optimizer': 'rmsprop', 'neurons': 43, 'batch_size': 42, 'epochs': 21}. Best is trial 0 with value: 2255.6711421187574.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 08:58:02,404] Trial 2 finished with value: 2254.902327413596 and parameters: {'optimizer': 'rmsprop', 'neurons': 68, 'batch_size': 47, 'epochs': 82}. Best is trial 2 with value: 2254.902327413596.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 08:59:02,821] Trial 3 finished with value: 2253.7992168423284 and parameters: {'optimizer': 'adam', 'neurons': 99, 'batch_size': 37, 'epochs': 63}. Best is trial 3 with value: 2253.7992168423284.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:00:05,776] Trial 4 finished with value: 2253.006090115664 and parameters: {'optimizer': 'adam', 'neurons': 121, 'batch_size': 53, 'epochs': 80}. Best is trial 4 with value: 2253.006090115664.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:01:34,187] Trial 5 finished with value: 2254.877044764769 and parameters: {'optimizer': 'adam', 'neurons': 104, 'batch_size': 32, 'epochs': 51}. Best is trial 4 with value: 2253.006090115664.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:01:52,387] Trial 6 finished with value: 2260.7255594967114 and parameters: {'optimizer': 'adam', 'neurons': 54, 'batch_size': 55, 'epochs': 26}. Best is trial 4 with value: 2253.006090115664.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:03:21,821] Trial 7 finished with value: 2253.499593491012 and parameters: {'optimizer': 'rmsprop', 'neurons': 51, 'batch_size': 27, 'epochs': 72}. Best is trial 4 with value: 2253.006090115664.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:05:42,089] Trial 8 finished with value: 2252.480927465534 and parameters: {'optimizer': 'adam', 'neurons': 127, 'batch_size': 22, 'epochs': 69}. Best is trial 8 with value: 2252.480927465534.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:06:54,771] Trial 9 finished with value: 2254.6647442811823 and parameters: {'optimizer': 'adam', 'neurons': 42, 'batch_size': 21, 'epochs': 44}. Best is trial 8 with value: 2252.480927465534.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:07:52,737] Trial 10 finished with value: 2252.142872192731 and parameters: {'optimizer': 'adam', 'neurons': 124, 'batch_size': 64, 'epochs': 99}. Best is trial 10 with value: 2252.142872192731.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:11:22,755] Trial 11 finished with value: 2249.1038366338084 and parameters: {'optimizer': 'adam', 'neurons': 127, 'batch_size': 16, 'epochs': 100}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:12:23,354] Trial 12 finished with value: 2252.3622141676296 and parameters: {'optimizer': 'adam', 'neurons': 111, 'batch_size': 61, 'epochs': 100}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:15:57,171] Trial 13 finished with value: 2249.4579214534833 and parameters: {'optimizer': 'adam', 'neurons': 82, 'batch_size': 16, 'epochs': 100}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:18:56,338] Trial 14 finished with value: 2250.5223471601216 and parameters: {'optimizer': 'adam', 'neurons': 80, 'batch_size': 17, 'epochs': 90}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:21:54,776] Trial 15 finished with value: 2250.4025316068564 and parameters: {'optimizer': 'adam', 'neurons': 84, 'batch_size': 17, 'epochs': 89}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:23:54,288] Trial 16 finished with value: 2250.8668674994015 and parameters: {'optimizer': 'adam', 'neurons': 69, 'batch_size': 26, 'epochs': 89}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:26:02,499] Trial 17 finished with value: 2250.484392976516 and parameters: {'optimizer': 'adam', 'neurons': 113, 'batch_size': 27, 'epochs': 99}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:28:07,821] Trial 18 finished with value: 2255.0626471140467 and parameters: {'optimizer': 'rmsprop', 'neurons': 91, 'batch_size': 16, 'epochs': 61}. Best is trial 11 with value: 2249.1038366338084.\n",
      "C:\\Users\\mbiel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 09:30:14,354] Trial 19 finished with value: 2254.8053084699727 and parameters: {'optimizer': 'adam', 'neurons': 64, 'batch_size': 22, 'epochs': 81}. Best is trial 11 with value: 2249.1038366338084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'optimizer': 'adam', 'neurons': 127, 'batch_size': 16, 'epochs': 100}\n",
      "Best MSE:  2249.1038366338084\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "    neurons = trial.suggest_int('neurons', 32, 128)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    epochs = trial.suggest_int('epochs', 10, 100)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict and calculate the MSE on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse  # We aim to minimize MSE\n",
    "\n",
    "# Create a study and optimize it\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best MSE: \", study.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fc00f",
   "metadata": {},
   "source": [
    "revised clean optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ced06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "    neurons = trial.suggest_int('neurons', 32, 128)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    epochs = trial.suggest_int('epochs', 10, 100)\n",
    "    \n",
    "    # Build the model using Input layer to avoid warnings\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_scaled.shape[1],)))  # Use Input layer instead of input_dim\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "# Create a study and optimize it\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best MSE: \", study.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8f9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0f1ed4d",
   "metadata": {},
   "source": [
    "Results : the hyperparameter optimisation process using Optuna has found the following best hyperparameters forthe model:\n",
    "\n",
    "Optimizer: adam\n",
    "Neurons: 127 (almost the upper boundary of the search space)\n",
    "Batch Size: 16 (smaller batch sizes often allow for more detailed updates to the weights)\n",
    "Epochs: 100 (the maximum value in the range, indicating that the model benefits from extended training)\n",
    "The best mean squared error (MSE) obtained was 2249.10.\n",
    "\n",
    "Interpretation:\n",
    "Optimizer (Adam): Adam is a widely used optimizer for neural networks as it combines the advantages of two other popular optimizers: AdaGrad and RMSProp. It adjusts the learning rate dynamically, which often results in faster and more stable convergence.\n",
    "Neurons (127): The model converged on a large number of neurons for the hidden layer. More neurons allow the network to capture more complex relationships between features but also increase the risk of overfitting if not regularized properly.\n",
    "Batch Size (16): A smaller batch size often means noisier but more frequent updates to the model weights, which can lead to faster convergence but also requires more computational resources.\n",
    "Epochs (100): Training for the maximum allowed number of epochs shows that the model likely continues to learn and reduce the error over time, which suggests that it might benefit from even longer training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c490e",
   "metadata": {},
   "source": [
    "Updated ANN Code with Best Hyperparameters:\n",
    "\n",
    "Changes Made Based on Best Hyperparameters:\n",
    "Neurons: Set to 127 in the first hidden layer.\n",
    "Batch Size: Set to 16.\n",
    "Epochs: Set to 100.\n",
    "Optimizer: Adam (as found to be the best in the hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "945cd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your dataset\n",
    "df = pd.read_excel('swimclass_rawdata.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbf518fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 220.5011 - val_loss: 13.3238\n",
      "Epoch 2/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 4.6661 - val_loss: 8.6396\n",
      "Epoch 3/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2.8487 - val_loss: 1.8805\n",
      "Epoch 4/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 2.8752 - val_loss: 1.6906\n",
      "Epoch 5/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2.3150 - val_loss: 4.5611\n",
      "Epoch 6/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2.0016 - val_loss: 1.7564\n",
      "Epoch 7/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2.2598 - val_loss: 4.1352\n",
      "Epoch 8/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.7847 - val_loss: 1.9785\n",
      "Epoch 9/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.7392 - val_loss: 1.1422\n",
      "Epoch 10/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2.2629 - val_loss: 0.8827\n",
      "Epoch 11/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.3024 - val_loss: 2.5697\n",
      "Epoch 12/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.5353 - val_loss: 2.5259\n",
      "Epoch 13/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.4062 - val_loss: 2.1284\n",
      "Epoch 14/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.3901 - val_loss: 1.1351\n",
      "Epoch 15/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.3245 - val_loss: 1.5759\n",
      "Epoch 16/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 1.6550 - val_loss: 0.7607\n",
      "Epoch 17/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.1090 - val_loss: 0.9047\n",
      "Epoch 18/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.4414 - val_loss: 1.8382\n",
      "Epoch 19/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.5596 - val_loss: 0.9561\n",
      "Epoch 20/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.1838 - val_loss: 0.9103\n",
      "Epoch 21/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.2283 - val_loss: 0.5152\n",
      "Epoch 22/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.1692 - val_loss: 1.2848\n",
      "Epoch 23/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.1386 - val_loss: 1.7566\n",
      "Epoch 24/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.1616 - val_loss: 1.8115\n",
      "Epoch 25/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.3201 - val_loss: 2.9063\n",
      "Epoch 26/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.0944 - val_loss: 1.1259\n",
      "Epoch 27/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.2272 - val_loss: 1.3025\n",
      "Epoch 28/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.1972 - val_loss: 0.5507\n",
      "Epoch 29/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.0008 - val_loss: 2.8842\n",
      "Epoch 30/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.1944 - val_loss: 0.8875\n",
      "Epoch 31/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.0641 - val_loss: 0.6031\n",
      "Epoch 32/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.0506 - val_loss: 1.5015\n",
      "Epoch 33/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.9449 - val_loss: 0.8991\n",
      "Epoch 34/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.9658 - val_loss: 0.6317\n",
      "Epoch 35/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 1.0463 - val_loss: 0.7359\n",
      "Epoch 36/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.9067 - val_loss: 1.5485\n",
      "Epoch 37/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.0895 - val_loss: 0.6667\n",
      "Epoch 38/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.0399 - val_loss: 0.8110\n",
      "Epoch 39/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 1.0339 - val_loss: 0.5826\n",
      "Epoch 40/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.9856 - val_loss: 0.5801\n",
      "Epoch 41/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 0.7374 - val_loss: 0.9309\n",
      "Epoch 42/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.9442 - val_loss: 0.8844\n",
      "Epoch 43/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8959 - val_loss: 0.7201\n",
      "Epoch 44/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.9234 - val_loss: 1.5596\n",
      "Epoch 45/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.8523 - val_loss: 1.2209\n",
      "Epoch 46/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.9009 - val_loss: 0.8943\n",
      "Epoch 47/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7894 - val_loss: 0.6114\n",
      "Epoch 48/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8063 - val_loss: 1.0078\n",
      "Epoch 49/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8381 - val_loss: 0.6780\n",
      "Epoch 50/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.9066 - val_loss: 0.4376\n",
      "Epoch 51/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8103 - val_loss: 2.3806\n",
      "Epoch 52/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.9135 - val_loss: 1.2909\n",
      "Epoch 53/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7586 - val_loss: 0.8367\n",
      "Epoch 54/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7995 - val_loss: 0.9874\n",
      "Epoch 55/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7496 - val_loss: 0.6921\n",
      "Epoch 56/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.6946 - val_loss: 0.9777\n",
      "Epoch 57/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8557 - val_loss: 1.4676\n",
      "Epoch 58/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.7142 - val_loss: 0.9388\n",
      "Epoch 59/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.7690 - val_loss: 0.9527\n",
      "Epoch 60/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8033 - val_loss: 0.8515\n",
      "Epoch 61/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7797 - val_loss: 1.2292\n",
      "Epoch 62/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.6750 - val_loss: 0.6020\n",
      "Epoch 63/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7946 - val_loss: 0.6972\n",
      "Epoch 64/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7361 - val_loss: 0.9464\n",
      "Epoch 65/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.7206 - val_loss: 2.0706\n",
      "Epoch 66/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8515 - val_loss: 0.9184\n",
      "Epoch 67/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.6787 - val_loss: 0.7367\n",
      "Epoch 68/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.8068 - val_loss: 0.8293\n",
      "Epoch 69/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7299 - val_loss: 1.6578\n",
      "Epoch 70/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.7345 - val_loss: 0.6432\n",
      "Epoch 71/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6421 - val_loss: 0.7169\n",
      "Epoch 72/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 0.6632 - val_loss: 1.2542\n",
      "Epoch 73/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.7380 - val_loss: 0.5844\n",
      "Epoch 74/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7244 - val_loss: 0.5209\n",
      "Epoch 75/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.6338 - val_loss: 0.5684\n",
      "Epoch 76/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6596 - val_loss: 0.5412\n",
      "Epoch 77/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6581 - val_loss: 1.2355\n",
      "Epoch 78/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.7640 - val_loss: 0.5190\n",
      "Epoch 79/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.5302 - val_loss: 0.7199\n",
      "Epoch 80/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.6756 - val_loss: 1.3005\n",
      "Epoch 81/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.5976 - val_loss: 0.5551\n",
      "Epoch 82/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6506 - val_loss: 0.8454\n",
      "Epoch 83/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6005 - val_loss: 0.4954\n",
      "Epoch 84/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 0.6762 - val_loss: 0.6418\n",
      "Epoch 85/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.5813 - val_loss: 0.8242\n",
      "Epoch 86/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.5859 - val_loss: 0.8177\n",
      "Epoch 87/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.5713 - val_loss: 0.5360\n",
      "Epoch 88/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.6711 - val_loss: 0.5857\n",
      "Epoch 89/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.5149 - val_loss: 0.4399\n",
      "Epoch 90/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 0.6239 - val_loss: 0.4922\n",
      "Epoch 91/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.5585 - val_loss: 1.1070\n",
      "Epoch 92/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.6821 - val_loss: 0.6848\n",
      "Epoch 93/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.5766 - val_loss: 0.6414\n",
      "Epoch 94/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6143 - val_loss: 0.5326\n",
      "Epoch 95/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.5845 - val_loss: 0.5539\n",
      "Epoch 96/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 0.5355 - val_loss: 0.9717\n",
      "Epoch 97/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 0.5645 - val_loss: 0.7855\n",
      "Epoch 98/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6055 - val_loss: 0.4992\n",
      "Epoch 99/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.6566 - val_loss: 0.7427\n",
      "Epoch 100/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 0.5344 - val_loss: 0.5653\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Classes used: 14 | Mean Squared Error: 0.82 | R-squared: 1.00\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                39.052773    0.052773\n",
      "30546                   105               105.231911    0.231911\n",
      "20806                    35                35.077221    0.077221\n",
      "39438                    30                30.023224    0.023224\n",
      "18229                   134               134.296921    0.296921\n",
      "Epoch 1/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 253.3767 - val_loss: 13.7815\n",
      "Epoch 2/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 14.3442 - val_loss: 11.3502\n",
      "Epoch 3/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 12.9319 - val_loss: 9.7951\n",
      "Epoch 4/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 12.3184 - val_loss: 9.2448\n",
      "Epoch 5/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 12.3478 - val_loss: 10.0450\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1509/1509 - 2s - 2ms/step - loss: 11.3922 - val_loss: 9.5188\n",
      "Epoch 7/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 11.7905 - val_loss: 8.9450\n",
      "Epoch 8/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 11.0213 - val_loss: 9.4896\n",
      "Epoch 9/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 11.5141 - val_loss: 11.3660\n",
      "Epoch 10/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 11.0538 - val_loss: 10.6995\n",
      "Epoch 11/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 10.7906 - val_loss: 9.8024\n",
      "Epoch 12/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 11.0463 - val_loss: 11.1155\n",
      "Epoch 13/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 10.6585 - val_loss: 8.7053\n",
      "Epoch 14/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 11.5449 - val_loss: 10.7944\n",
      "Epoch 15/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 10.1927 - val_loss: 9.0600\n",
      "Epoch 16/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 10.5383 - val_loss: 10.2212\n",
      "Epoch 17/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 10.4383 - val_loss: 8.1885\n",
      "Epoch 18/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 10.2943 - val_loss: 14.8674\n",
      "Epoch 19/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 10.2250 - val_loss: 12.7823\n",
      "Epoch 20/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 10.5752 - val_loss: 13.8686\n",
      "Epoch 21/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 10.5686 - val_loss: 8.2287\n",
      "Epoch 22/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 10.0699 - val_loss: 10.4899\n",
      "Epoch 23/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 10.0083 - val_loss: 9.3207\n",
      "Epoch 24/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.8270 - val_loss: 8.9000\n",
      "Epoch 25/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.7394 - val_loss: 7.7923\n",
      "Epoch 26/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 9.7797 - val_loss: 8.8925\n",
      "Epoch 27/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.9958 - val_loss: 8.8361\n",
      "Epoch 28/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.5863 - val_loss: 9.1862\n",
      "Epoch 29/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.6089 - val_loss: 9.7153\n",
      "Epoch 30/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 9.9304 - val_loss: 9.3028\n",
      "Epoch 31/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.2820 - val_loss: 9.0068\n",
      "Epoch 32/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.5183 - val_loss: 7.9952\n",
      "Epoch 33/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.8907 - val_loss: 8.7357\n",
      "Epoch 34/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.2081 - val_loss: 13.2101\n",
      "Epoch 35/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.7034 - val_loss: 7.6963\n",
      "Epoch 36/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 9.0437 - val_loss: 8.1000\n",
      "Epoch 37/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.1131 - val_loss: 8.7844\n",
      "Epoch 38/100\n",
      "1509/1509 - 5s - 4ms/step - loss: 9.0604 - val_loss: 9.3130\n",
      "Epoch 39/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.4886 - val_loss: 8.1624\n",
      "Epoch 40/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.8819 - val_loss: 10.9091\n",
      "Epoch 41/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 8.9917 - val_loss: 8.3693\n",
      "Epoch 42/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.3085 - val_loss: 8.9022\n",
      "Epoch 43/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.3314 - val_loss: 7.8673\n",
      "Epoch 44/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.9621 - val_loss: 12.8429\n",
      "Epoch 45/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.0682 - val_loss: 13.3329\n",
      "Epoch 46/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 9.0222 - val_loss: 10.2245\n",
      "Epoch 47/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 8.9132 - val_loss: 8.3191\n",
      "Epoch 48/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.8231 - val_loss: 11.7881\n",
      "Epoch 49/100\n",
      "1509/1509 - 4s - 2ms/step - loss: 8.8223 - val_loss: 11.8043\n",
      "Epoch 50/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.6990 - val_loss: 8.4893\n",
      "Epoch 51/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 8.6737 - val_loss: 9.9282\n",
      "Epoch 52/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.6310 - val_loss: 9.6417\n",
      "Epoch 53/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 8.5870 - val_loss: 11.6500\n",
      "Epoch 54/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.4702 - val_loss: 10.0809\n",
      "Epoch 55/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 8.4095 - val_loss: 8.7920\n",
      "Epoch 56/100\n",
      "1509/1509 - 6s - 4ms/step - loss: 8.7394 - val_loss: 8.3468\n",
      "Epoch 57/100\n",
      "1509/1509 - 4s - 2ms/step - loss: 8.4700 - val_loss: 12.8991\n",
      "Epoch 58/100\n",
      "1509/1509 - 5s - 4ms/step - loss: 8.7269 - val_loss: 8.6526\n",
      "Epoch 59/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 8.6073 - val_loss: 8.8725\n",
      "Epoch 60/100\n",
      "1509/1509 - 4s - 2ms/step - loss: 8.4562 - val_loss: 12.1696\n",
      "Epoch 61/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 7.9839 - val_loss: 8.4851\n",
      "Epoch 62/100\n",
      "1509/1509 - 4s - 2ms/step - loss: 8.4917 - val_loss: 8.4401\n",
      "Epoch 63/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 8.3240 - val_loss: 8.7642\n",
      "Epoch 64/100\n",
      "1509/1509 - 4s - 2ms/step - loss: 8.5169 - val_loss: 8.0173\n",
      "Epoch 65/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 8.6102 - val_loss: 8.8932\n",
      "Epoch 66/100\n",
      "1509/1509 - 5s - 4ms/step - loss: 8.4000 - val_loss: 8.9331\n",
      "Epoch 67/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 8.2910 - val_loss: 10.1684\n",
      "Epoch 68/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 8.6167 - val_loss: 8.9804\n",
      "Epoch 69/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.9702 - val_loss: 11.6307\n",
      "Epoch 70/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 8.5315 - val_loss: 10.3597\n",
      "Epoch 71/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.0703 - val_loss: 9.4681\n",
      "Epoch 72/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.1038 - val_loss: 8.7823\n",
      "Epoch 73/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 8.1068 - val_loss: 10.1200\n",
      "Epoch 74/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.8051 - val_loss: 8.7238\n",
      "Epoch 75/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 8.4628 - val_loss: 9.0727\n",
      "Epoch 76/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.4668 - val_loss: 9.9496\n",
      "Epoch 77/100\n",
      "1509/1509 - 6s - 4ms/step - loss: 8.0953 - val_loss: 21.5559\n",
      "Epoch 78/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.1958 - val_loss: 9.4619\n",
      "Epoch 79/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.5663 - val_loss: 8.4608\n",
      "Epoch 80/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.3214 - val_loss: 9.0339\n",
      "Epoch 81/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.8037 - val_loss: 8.3805\n",
      "Epoch 82/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.1770 - val_loss: 8.5346\n",
      "Epoch 83/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 7.7988 - val_loss: 14.5020\n",
      "Epoch 84/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.8143 - val_loss: 8.7887\n",
      "Epoch 85/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.6030 - val_loss: 8.4889\n",
      "Epoch 86/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.7662 - val_loss: 9.0325\n",
      "Epoch 87/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 8.0724 - val_loss: 8.9267\n",
      "Epoch 88/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.6467 - val_loss: 8.5024\n",
      "Epoch 89/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.6675 - val_loss: 10.3865\n",
      "Epoch 90/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.6090 - val_loss: 12.8082\n",
      "Epoch 91/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.3578 - val_loss: 12.2894\n",
      "Epoch 92/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.4984 - val_loss: 10.8287\n",
      "Epoch 93/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.8626 - val_loss: 10.2385\n",
      "Epoch 94/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.6035 - val_loss: 10.8369\n",
      "Epoch 95/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.0128 - val_loss: 14.7596\n",
      "Epoch 96/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.5650 - val_loss: 7.9880\n",
      "Epoch 97/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.3951 - val_loss: 9.3002\n",
      "Epoch 98/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.2705 - val_loss: 10.8899\n",
      "Epoch 99/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 7.0227 - val_loss: 14.2305\n",
      "Epoch 100/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 7.2633 - val_loss: 9.6316\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Classes used: 11 | Mean Squared Error: 8.21 | R-squared: 1.00\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                38.634995   -0.365005\n",
      "30546                   105               104.251366   -0.748634\n",
      "20806                    35                35.083950    0.083950\n",
      "39438                    30                29.920155   -0.079845\n",
      "18229                   134               133.617310   -0.382690\n",
      "Epoch 1/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 574.4391 - val_loss: 264.4303\n",
      "Epoch 2/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 275.3090 - val_loss: 241.3925\n",
      "Epoch 3/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 265.5353 - val_loss: 249.1582\n",
      "Epoch 4/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 261.7219 - val_loss: 237.7760\n",
      "Epoch 5/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 261.7285 - val_loss: 257.6663\n",
      "Epoch 6/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 260.9960 - val_loss: 232.9456\n",
      "Epoch 7/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 256.4421 - val_loss: 233.1351\n",
      "Epoch 8/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 257.4458 - val_loss: 237.4372\n",
      "Epoch 9/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 255.5110 - val_loss: 247.0800\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1509/1509 - 3s - 2ms/step - loss: 258.3735 - val_loss: 236.2105\n",
      "Epoch 11/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 255.2215 - val_loss: 246.5443\n",
      "Epoch 12/100\n",
      "1509/1509 - 5s - 4ms/step - loss: 256.0258 - val_loss: 239.5229\n",
      "Epoch 13/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 254.9734 - val_loss: 232.1377\n",
      "Epoch 14/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 255.2583 - val_loss: 232.6532\n",
      "Epoch 15/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 251.9284 - val_loss: 245.5881\n",
      "Epoch 16/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 253.2455 - val_loss: 238.2728\n",
      "Epoch 17/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 251.8938 - val_loss: 254.2750\n",
      "Epoch 18/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 251.5737 - val_loss: 236.9374\n",
      "Epoch 19/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 252.0130 - val_loss: 244.9591\n",
      "Epoch 20/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 250.9172 - val_loss: 233.9496\n",
      "Epoch 21/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 247.9893 - val_loss: 234.2617\n",
      "Epoch 22/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 251.0495 - val_loss: 238.7755\n",
      "Epoch 23/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 250.0016 - val_loss: 231.1400\n",
      "Epoch 24/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 246.9114 - val_loss: 245.9728\n",
      "Epoch 25/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 248.0159 - val_loss: 224.2171\n",
      "Epoch 26/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 247.6922 - val_loss: 232.2899\n",
      "Epoch 27/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 246.6939 - val_loss: 235.4151\n",
      "Epoch 28/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 247.5677 - val_loss: 234.6883\n",
      "Epoch 29/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 245.9770 - val_loss: 229.8268\n",
      "Epoch 30/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 243.6550 - val_loss: 223.5098\n",
      "Epoch 31/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 244.2040 - val_loss: 241.0805\n",
      "Epoch 32/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 243.5492 - val_loss: 220.5622\n",
      "Epoch 33/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 241.5906 - val_loss: 224.2167\n",
      "Epoch 34/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 242.0972 - val_loss: 225.2266\n",
      "Epoch 35/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 241.3136 - val_loss: 221.7330\n",
      "Epoch 36/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 239.8312 - val_loss: 218.8867\n",
      "Epoch 37/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 240.7449 - val_loss: 223.6914\n",
      "Epoch 38/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 239.4538 - val_loss: 223.9600\n",
      "Epoch 39/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 239.3309 - val_loss: 217.9131\n",
      "Epoch 40/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 239.2770 - val_loss: 222.5371\n",
      "Epoch 41/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 237.2139 - val_loss: 232.1756\n",
      "Epoch 42/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 236.8449 - val_loss: 220.1877\n",
      "Epoch 43/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 237.0795 - val_loss: 226.9585\n",
      "Epoch 44/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 235.5698 - val_loss: 217.4258\n",
      "Epoch 45/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 234.6933 - val_loss: 222.2052\n",
      "Epoch 46/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 235.6563 - val_loss: 226.6625\n",
      "Epoch 47/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 232.8923 - val_loss: 218.8435\n",
      "Epoch 48/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 234.0548 - val_loss: 232.1525\n",
      "Epoch 49/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 233.6838 - val_loss: 220.7263\n",
      "Epoch 50/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 233.9095 - val_loss: 215.9680\n",
      "Epoch 51/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 233.8765 - val_loss: 210.4736\n",
      "Epoch 52/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 234.0807 - val_loss: 225.6587\n",
      "Epoch 53/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 232.5945 - val_loss: 223.6569\n",
      "Epoch 54/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 231.4398 - val_loss: 227.2413\n",
      "Epoch 55/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 232.1120 - val_loss: 223.1013\n",
      "Epoch 56/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 231.7109 - val_loss: 219.8816\n",
      "Epoch 57/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 230.8543 - val_loss: 229.4898\n",
      "Epoch 58/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 230.1820 - val_loss: 217.7821\n",
      "Epoch 59/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 230.2711 - val_loss: 217.9548\n",
      "Epoch 60/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 229.5658 - val_loss: 231.6661\n",
      "Epoch 61/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 227.7411 - val_loss: 228.4541\n",
      "Epoch 62/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 229.0835 - val_loss: 217.2720\n",
      "Epoch 63/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 229.1951 - val_loss: 221.4590\n",
      "Epoch 64/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 228.3972 - val_loss: 217.5474\n",
      "Epoch 65/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 227.9249 - val_loss: 219.1406\n",
      "Epoch 66/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 226.4880 - val_loss: 212.8841\n",
      "Epoch 67/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 225.7121 - val_loss: 219.2827\n",
      "Epoch 68/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 226.0133 - val_loss: 213.9355\n",
      "Epoch 69/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 227.4395 - val_loss: 221.6998\n",
      "Epoch 70/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 225.8471 - val_loss: 220.2216\n",
      "Epoch 71/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 225.5987 - val_loss: 215.5818\n",
      "Epoch 72/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 226.6068 - val_loss: 215.7123\n",
      "Epoch 73/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 224.5220 - val_loss: 231.3929\n",
      "Epoch 74/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 224.0090 - val_loss: 213.0515\n",
      "Epoch 75/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 224.4919 - val_loss: 222.1351\n",
      "Epoch 76/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 223.7377 - val_loss: 218.3969\n",
      "Epoch 77/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 224.0930 - val_loss: 214.9161\n",
      "Epoch 78/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 223.1643 - val_loss: 222.3099\n",
      "Epoch 79/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 224.7589 - val_loss: 224.0787\n",
      "Epoch 80/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 223.1210 - val_loss: 221.0302\n",
      "Epoch 81/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 223.2120 - val_loss: 215.9453\n",
      "Epoch 82/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 222.4002 - val_loss: 228.2320\n",
      "Epoch 83/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 221.6578 - val_loss: 214.6563\n",
      "Epoch 84/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 220.6365 - val_loss: 213.6526\n",
      "Epoch 85/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 222.3076 - val_loss: 212.0886\n",
      "Epoch 86/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 221.6921 - val_loss: 221.6796\n",
      "Epoch 87/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 221.8206 - val_loss: 212.2503\n",
      "Epoch 88/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 219.3869 - val_loss: 219.4056\n",
      "Epoch 89/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 222.0911 - val_loss: 216.6739\n",
      "Epoch 90/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 220.2846 - val_loss: 221.0667\n",
      "Epoch 91/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 219.4508 - val_loss: 226.4770\n",
      "Epoch 92/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 216.9248 - val_loss: 225.0441\n",
      "Epoch 93/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 219.4058 - val_loss: 218.0931\n",
      "Epoch 94/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 219.4722 - val_loss: 210.0612\n",
      "Epoch 95/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 219.8733 - val_loss: 219.4182\n",
      "Epoch 96/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 217.5983 - val_loss: 219.5127\n",
      "Epoch 97/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 216.6522 - val_loss: 219.2325\n",
      "Epoch 98/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 217.2939 - val_loss: 226.5880\n",
      "Epoch 99/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 217.8042 - val_loss: 222.3671\n",
      "Epoch 100/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 218.7979 - val_loss: 218.2510\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Classes used: 8 | Mean Squared Error: 212.28 | R-squared: 0.92\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                40.080444    1.080444\n",
      "30546                   105               117.914139   12.914139\n",
      "20806                    35                33.823544   -1.176456\n",
      "39438                    30                30.939425    0.939425\n",
      "18229                   134               131.397751   -2.602249\n",
      "Epoch 1/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 1063.9427 - val_loss: 842.7534\n",
      "Epoch 2/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 851.0853 - val_loss: 826.7385\n",
      "Epoch 3/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 834.8362 - val_loss: 812.9247\n",
      "Epoch 4/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 820.7498 - val_loss: 817.7567\n",
      "Epoch 5/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 813.5517 - val_loss: 792.7770\n",
      "Epoch 6/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 806.0679 - val_loss: 803.9286\n",
      "Epoch 7/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 802.5552 - val_loss: 769.6587\n",
      "Epoch 8/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 799.5015 - val_loss: 763.7775\n",
      "Epoch 9/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 795.5025 - val_loss: 784.2489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 793.2783 - val_loss: 770.9091\n",
      "Epoch 11/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 787.7170 - val_loss: 753.2531\n",
      "Epoch 12/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 788.0262 - val_loss: 772.6821\n",
      "Epoch 13/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 787.5667 - val_loss: 768.2220\n",
      "Epoch 14/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 783.2938 - val_loss: 753.4684\n",
      "Epoch 15/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 780.1913 - val_loss: 788.0505\n",
      "Epoch 16/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 784.5383 - val_loss: 756.8053\n",
      "Epoch 17/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 781.4944 - val_loss: 766.0422\n",
      "Epoch 18/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 781.6238 - val_loss: 743.0609\n",
      "Epoch 19/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 779.9602 - val_loss: 795.2603\n",
      "Epoch 20/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 775.9935 - val_loss: 746.7892\n",
      "Epoch 21/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 778.0149 - val_loss: 759.0793\n",
      "Epoch 22/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 780.6810 - val_loss: 756.1238\n",
      "Epoch 23/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 774.5256 - val_loss: 793.4753\n",
      "Epoch 24/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 775.4420 - val_loss: 758.3995\n",
      "Epoch 25/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 776.1469 - val_loss: 749.3254\n",
      "Epoch 26/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 774.2715 - val_loss: 742.5275\n",
      "Epoch 27/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 773.8237 - val_loss: 749.1540\n",
      "Epoch 28/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 771.3495 - val_loss: 775.8990\n",
      "Epoch 29/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 772.5011 - val_loss: 742.1381\n",
      "Epoch 30/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 772.2926 - val_loss: 737.3552\n",
      "Epoch 31/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 772.9572 - val_loss: 785.7999\n",
      "Epoch 32/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 774.1526 - val_loss: 752.3379\n",
      "Epoch 33/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 770.5798 - val_loss: 758.6287\n",
      "Epoch 34/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 770.1833 - val_loss: 744.4338\n",
      "Epoch 35/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 770.2974 - val_loss: 750.4014\n",
      "Epoch 36/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 767.4805 - val_loss: 754.7919\n",
      "Epoch 37/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 769.5286 - val_loss: 745.6952\n",
      "Epoch 38/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 768.9871 - val_loss: 738.3132\n",
      "Epoch 39/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 771.1306 - val_loss: 743.6945\n",
      "Epoch 40/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 769.0290 - val_loss: 744.6436\n",
      "Epoch 41/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 767.6860 - val_loss: 756.2275\n",
      "Epoch 42/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 769.7207 - val_loss: 747.5171\n",
      "Epoch 43/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 767.8289 - val_loss: 737.2929\n",
      "Epoch 44/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 764.9129 - val_loss: 742.5699\n",
      "Epoch 45/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 763.3121 - val_loss: 750.6228\n",
      "Epoch 46/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 763.0630 - val_loss: 748.0391\n",
      "Epoch 47/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 769.5080 - val_loss: 745.0352\n",
      "Epoch 48/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 763.7485 - val_loss: 772.1232\n",
      "Epoch 49/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 764.6318 - val_loss: 735.9564\n",
      "Epoch 50/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 764.3443 - val_loss: 753.0028\n",
      "Epoch 51/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 763.9569 - val_loss: 752.1834\n",
      "Epoch 52/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 766.0720 - val_loss: 747.6097\n",
      "Epoch 53/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 764.2727 - val_loss: 747.8816\n",
      "Epoch 54/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 762.5626 - val_loss: 749.4313\n",
      "Epoch 55/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 761.2751 - val_loss: 754.0922\n",
      "Epoch 56/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 760.5344 - val_loss: 730.3774\n",
      "Epoch 57/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 761.4251 - val_loss: 742.5215\n",
      "Epoch 58/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 761.6122 - val_loss: 739.5421\n",
      "Epoch 59/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 762.4864 - val_loss: 745.7853\n",
      "Epoch 60/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 763.2595 - val_loss: 735.2662\n",
      "Epoch 61/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 762.2432 - val_loss: 758.8527\n",
      "Epoch 62/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 760.4471 - val_loss: 752.3833\n",
      "Epoch 63/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 760.9620 - val_loss: 733.1866\n",
      "Epoch 64/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 760.8954 - val_loss: 733.6103\n",
      "Epoch 65/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 757.7199 - val_loss: 736.9136\n",
      "Epoch 66/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 759.5735 - val_loss: 750.2137\n",
      "Epoch 67/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 757.5031 - val_loss: 771.3554\n",
      "Epoch 68/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 757.8095 - val_loss: 741.8866\n",
      "Epoch 69/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 761.7770 - val_loss: 737.9484\n",
      "Epoch 70/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 758.9120 - val_loss: 746.2842\n",
      "Epoch 71/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 758.0056 - val_loss: 773.8060\n",
      "Epoch 72/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 759.6451 - val_loss: 728.8661\n",
      "Epoch 73/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 756.2082 - val_loss: 730.5546\n",
      "Epoch 74/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 757.8548 - val_loss: 741.7075\n",
      "Epoch 75/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 756.6990 - val_loss: 747.0762\n",
      "Epoch 76/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 756.9852 - val_loss: 739.0509\n",
      "Epoch 77/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 757.1325 - val_loss: 737.5667\n",
      "Epoch 78/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 756.4677 - val_loss: 733.0816\n",
      "Epoch 79/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 754.4130 - val_loss: 745.6536\n",
      "Epoch 80/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 753.2283 - val_loss: 732.2525\n",
      "Epoch 81/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 753.8456 - val_loss: 739.8289\n",
      "Epoch 82/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 755.1681 - val_loss: 744.9808\n",
      "Epoch 83/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 753.8054 - val_loss: 744.7932\n",
      "Epoch 84/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 753.3939 - val_loss: 735.5880\n",
      "Epoch 85/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 755.1722 - val_loss: 748.9572\n",
      "Epoch 86/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 754.5432 - val_loss: 731.9715\n",
      "Epoch 87/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 752.7143 - val_loss: 732.4938\n",
      "Epoch 88/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 751.8937 - val_loss: 740.6746\n",
      "Epoch 89/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 755.8364 - val_loss: 729.0869\n",
      "Epoch 90/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 756.8891 - val_loss: 749.4456\n",
      "Epoch 91/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 751.3367 - val_loss: 739.7460\n",
      "Epoch 92/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 754.3872 - val_loss: 741.9338\n",
      "Epoch 93/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 753.2435 - val_loss: 730.7077\n",
      "Epoch 94/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 750.6031 - val_loss: 734.2908\n",
      "Epoch 95/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 749.6258 - val_loss: 733.0264\n",
      "Epoch 96/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 752.1841 - val_loss: 730.2382\n",
      "Epoch 97/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 752.4047 - val_loss: 733.6895\n",
      "Epoch 98/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 753.6927 - val_loss: 757.9427\n",
      "Epoch 99/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 753.6296 - val_loss: 732.5540\n",
      "Epoch 100/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 756.0192 - val_loss: 730.6647\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Classes used: 5 | Mean Squared Error: 702.24 | R-squared: 0.74\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                50.266125   11.266125\n",
      "30546                   105               118.840439   13.840439\n",
      "20806                    35                56.597195   21.597195\n",
      "39438                    30                38.875069    8.875069\n",
      "18229                   134               135.221466    1.221466\n",
      "Epoch 1/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 2490.7888 - val_loss: 2290.5747\n",
      "Epoch 2/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2322.2278 - val_loss: 2286.3806\n",
      "Epoch 3/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2319.0947 - val_loss: 2276.9307\n",
      "Epoch 4/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2318.1819 - val_loss: 2280.4275\n",
      "Epoch 5/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2314.3999 - val_loss: 2278.4805\n",
      "Epoch 6/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2315.2061 - val_loss: 2277.8042\n",
      "Epoch 7/100\n",
      "1509/1509 - 4s - 3ms/step - loss: 2313.8635 - val_loss: 2286.6602\n",
      "Epoch 8/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2312.4861 - val_loss: 2275.5339\n",
      "Epoch 9/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2315.7500 - val_loss: 2269.5703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2312.2263 - val_loss: 2289.5181\n",
      "Epoch 11/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2313.6382 - val_loss: 2278.5383\n",
      "Epoch 12/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2311.6509 - val_loss: 2273.5703\n",
      "Epoch 13/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2310.6736 - val_loss: 2270.8738\n",
      "Epoch 14/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2313.0371 - val_loss: 2269.6497\n",
      "Epoch 15/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2310.4836 - val_loss: 2292.1177\n",
      "Epoch 16/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2311.6963 - val_loss: 2276.3057\n",
      "Epoch 17/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2310.4504 - val_loss: 2268.5686\n",
      "Epoch 18/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2307.8704 - val_loss: 2267.5649\n",
      "Epoch 19/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2307.8879 - val_loss: 2266.3069\n",
      "Epoch 20/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2309.3774 - val_loss: 2275.7422\n",
      "Epoch 21/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2307.7131 - val_loss: 2273.6089\n",
      "Epoch 22/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2308.6208 - val_loss: 2278.7390\n",
      "Epoch 23/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2307.7402 - val_loss: 2279.1946\n",
      "Epoch 24/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2310.0332 - val_loss: 2270.7292\n",
      "Epoch 25/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 2308.7893 - val_loss: 2269.0710\n",
      "Epoch 26/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2307.9111 - val_loss: 2275.0891\n",
      "Epoch 27/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2308.4236 - val_loss: 2284.7988\n",
      "Epoch 28/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2308.9968 - val_loss: 2267.6111\n",
      "Epoch 29/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2305.3733 - val_loss: 2279.1633\n",
      "Epoch 30/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2305.8569 - val_loss: 2262.4893\n",
      "Epoch 31/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2307.4094 - val_loss: 2267.0291\n",
      "Epoch 32/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2306.8687 - val_loss: 2264.4272\n",
      "Epoch 33/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2306.3823 - val_loss: 2274.4609\n",
      "Epoch 34/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2305.2319 - val_loss: 2262.9883\n",
      "Epoch 35/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2305.9062 - val_loss: 2265.8264\n",
      "Epoch 36/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2305.3462 - val_loss: 2266.8745\n",
      "Epoch 37/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2302.7981 - val_loss: 2273.1243\n",
      "Epoch 38/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.6099 - val_loss: 2280.3892\n",
      "Epoch 39/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2306.4319 - val_loss: 2261.7063\n",
      "Epoch 40/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2304.2939 - val_loss: 2298.3174\n",
      "Epoch 41/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.0381 - val_loss: 2290.6643\n",
      "Epoch 42/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2304.3047 - val_loss: 2263.4282\n",
      "Epoch 43/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2304.3464 - val_loss: 2263.3220\n",
      "Epoch 44/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2306.1938 - val_loss: 2264.9414\n",
      "Epoch 45/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2306.6396 - val_loss: 2270.9802\n",
      "Epoch 46/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2305.5547 - val_loss: 2270.7493\n",
      "Epoch 47/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.6387 - val_loss: 2269.8821\n",
      "Epoch 48/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.4797 - val_loss: 2266.6538\n",
      "Epoch 49/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2302.5459 - val_loss: 2274.8459\n",
      "Epoch 50/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2304.2651 - val_loss: 2265.5159\n",
      "Epoch 51/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2302.1697 - val_loss: 2261.4348\n",
      "Epoch 52/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2303.1570 - val_loss: 2274.8240\n",
      "Epoch 53/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.7363 - val_loss: 2261.0513\n",
      "Epoch 54/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2303.6687 - val_loss: 2285.1289\n",
      "Epoch 55/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2304.7800 - val_loss: 2266.1541\n",
      "Epoch 56/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2302.4858 - val_loss: 2262.9917\n",
      "Epoch 57/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.3088 - val_loss: 2261.9934\n",
      "Epoch 58/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.7886 - val_loss: 2261.6333\n",
      "Epoch 59/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2304.3513 - val_loss: 2265.0691\n",
      "Epoch 60/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2303.0940 - val_loss: 2266.3196\n",
      "Epoch 61/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2301.2710 - val_loss: 2260.7676\n",
      "Epoch 62/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2302.6694 - val_loss: 2259.0767\n",
      "Epoch 63/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2302.5708 - val_loss: 2260.9160\n",
      "Epoch 64/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2303.7419 - val_loss: 2258.7959\n",
      "Epoch 65/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2302.4285 - val_loss: 2259.2263\n",
      "Epoch 66/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2303.8169 - val_loss: 2264.2437\n",
      "Epoch 67/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2302.8679 - val_loss: 2264.6357\n",
      "Epoch 68/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2300.9629 - val_loss: 2259.0947\n",
      "Epoch 69/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2301.7502 - val_loss: 2264.0962\n",
      "Epoch 70/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2301.0090 - val_loss: 2271.5491\n",
      "Epoch 71/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2302.0715 - val_loss: 2265.7422\n",
      "Epoch 72/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2303.8608 - val_loss: 2256.9890\n",
      "Epoch 73/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2301.2544 - val_loss: 2263.9636\n",
      "Epoch 74/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2301.7114 - val_loss: 2259.2222\n",
      "Epoch 75/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2300.2332 - val_loss: 2259.0122\n",
      "Epoch 76/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2299.9893 - val_loss: 2265.5042\n",
      "Epoch 77/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2302.2559 - val_loss: 2267.4336\n",
      "Epoch 78/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2301.8691 - val_loss: 2259.7954\n",
      "Epoch 79/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2300.1926 - val_loss: 2283.9480\n",
      "Epoch 80/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2299.6797 - val_loss: 2263.7456\n",
      "Epoch 81/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2300.0984 - val_loss: 2261.8435\n",
      "Epoch 82/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2301.3835 - val_loss: 2264.6230\n",
      "Epoch 83/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2300.7773 - val_loss: 2257.8923\n",
      "Epoch 84/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2300.1589 - val_loss: 2260.2263\n",
      "Epoch 85/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2300.1633 - val_loss: 2259.4285\n",
      "Epoch 86/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2300.6631 - val_loss: 2287.0664\n",
      "Epoch 87/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2301.1580 - val_loss: 2261.4304\n",
      "Epoch 88/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2302.4746 - val_loss: 2261.9502\n",
      "Epoch 89/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 2298.3586 - val_loss: 2262.1355\n",
      "Epoch 90/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2300.3496 - val_loss: 2262.1897\n",
      "Epoch 91/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2299.2080 - val_loss: 2265.0093\n",
      "Epoch 92/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2298.9121 - val_loss: 2257.4187\n",
      "Epoch 93/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2300.9202 - val_loss: 2265.9287\n",
      "Epoch 94/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2300.2131 - val_loss: 2264.9072\n",
      "Epoch 95/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2298.6304 - val_loss: 2267.0176\n",
      "Epoch 96/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2300.9133 - val_loss: 2276.1790\n",
      "Epoch 97/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2301.2437 - val_loss: 2258.9749\n",
      "Epoch 98/100\n",
      "1509/1509 - 2s - 2ms/step - loss: 2300.1509 - val_loss: 2258.5825\n",
      "Epoch 99/100\n",
      "1509/1509 - 3s - 2ms/step - loss: 2299.5344 - val_loss: 2259.9644\n",
      "Epoch 100/100\n",
      "1509/1509 - 5s - 3ms/step - loss: 2299.1206 - val_loss: 2262.1934\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Classes used: 2 | Mean Squared Error: 2253.07 | R-squared: 0.16\n",
      "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
      "20075                    39                47.526028    8.526028\n",
      "30546                   105                38.257729  -66.742271\n",
      "20806                    35                47.526028   12.526028\n",
      "39438                    30                47.526028   17.526028\n",
      "18229                   134                97.894379  -36.105621\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data for leavers (ignore current members for now)\n",
    "leavers_df = df[df['Current_member'] == 0].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Step 3: Define the class columns\n",
    "lesson_columns = ['PreSchool 1', 'PreSchool 2', 'PreSchool 3', 'PreSchool 4', 'PreSchool 5',\n",
    "                  'PreSchool 6', 'Academy 1', 'Academy 2', 'Academy 3', 'Academy 4', \n",
    "                  'Academy 5', 'Academy 6', 'BEGINNERS', 'INTERMEDIATE']\n",
    "\n",
    "# Step 4: Calculate Variance to Median (VTM) for each class column\n",
    "for col in lesson_columns:\n",
    "    median_value = leavers_df.loc[leavers_df[col] > 0, col].median()  # Exclude zero values\n",
    "    leavers_df.loc[:, f'{col}_VTM'] = leavers_df[col] - median_value  # Proper use of .loc to avoid warnings\n",
    "\n",
    "# Step 5: Train on progressively reduced class data\n",
    "for classes_used in range(len(lesson_columns), 1, -3):  # Remove class data progressively, 3 at a time\n",
    "    selected_classes = lesson_columns[:classes_used]\n",
    "    features = [f'{col}_VTM' for col in selected_classes] + ['TOTAL QUANTITY OF LESSONS']\n",
    "    \n",
    "    # Step 6: Define the target (Total lessons we want to predict)\n",
    "    leavers_df['PAST_LESSONS'] = leavers_df[lesson_columns].sum(axis=1)  # Known lessons taken so far\n",
    "    \n",
    "    # Remove TOTAL_QUANTITY_OF_LESSONS for training\n",
    "    X_leavers = leavers_df[features].drop(columns=['TOTAL QUANTITY OF LESSONS'])\n",
    "    y_leavers = leavers_df['TOTAL QUANTITY OF LESSONS']  # Total lessons as target\n",
    "    \n",
    "    # Step 7: Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_leavers, y_leavers, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Step 8: Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 9: Build the ANN model using the best hyperparameters from Optuna\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_scaled.shape[1],)))  # Use Input(shape) for the first layer\n",
    "    model.add(Dense(127, activation='relu'))  # Use 127 neurons based on Optuna results\n",
    "    model.add(Dense(64, activation='relu'))   # 2nd hidden layer\n",
    "    model.add(Dense(32, activation='relu'))   # 3rd hidden layer\n",
    "    model.add(Dense(1))  # Output layer for regression (single target)\n",
    "\n",
    "    # Step 10: Compile the model using Adam optimizer\n",
    "    optimizer = Adam()\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Step 11: Train the model using the best batch size and epochs\n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, verbose=2, validation_split=0.2)\n",
    "    \n",
    "    # Step 12: Make predictions and evaluate the model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results for each iteration\n",
    "    print(f\"Classes used: {classes_used} | Mean Squared Error: {mse:.2f} | R-squared: {r2:.2f}\")\n",
    "    \n",
    "    # Optional: Compare actual vs predicted total lessons\n",
    "    results_df = pd.DataFrame({'Actual_TOTAL_LESSONS': y_test, 'Predicted_TOTAL_LESSONS': y_pred.flatten()})\n",
    "    results_df['DIFFERENCE'] = results_df['Predicted_TOTAL_LESSONS'] - results_df['Actual_TOTAL_LESSONS']\n",
    "    print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea468185",
   "metadata": {},
   "source": [
    "Results summary from ANN with hyerparameters post tuned attempt\n",
    "\n",
    "Classes used: 14 | Mean Squared Error: 0.82 | R-squared: 1.00\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                39.052773    0.052773\n",
    "30546                   105               105.231911    0.231911\n",
    "20806                    35                35.077221    0.077221\n",
    "39438                    30                30.023224    0.023224\n",
    "18229                   134               134.296921    0.296921\n",
    "\n",
    "\n",
    "Classes used: 11 | Mean Squared Error: 8.21 | R-squared: 1.00\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                38.634995   -0.365005\n",
    "30546                   105               104.251366   -0.748634\n",
    "20806                    35                35.083950    0.083950\n",
    "39438                    30                29.920155   -0.079845\n",
    "18229                   134               133.617310   -0.382690\n",
    "\n",
    "Classes used: 8 | Mean Squared Error: 212.28 | R-squared: 0.92\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                40.080444    1.080444\n",
    "30546                   105               117.914139   12.914139\n",
    "20806                    35                33.823544   -1.176456\n",
    "39438                    30                30.939425    0.939425\n",
    "18229                   134               131.397751   -2.602249\n",
    "\n",
    "Classes used: 5 | Mean Squared Error: 702.24 | R-squared: 0.74\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                50.266125   11.266125\n",
    "30546                   105               118.840439   13.840439\n",
    "20806                    35                56.597195   21.597195\n",
    "39438                    30                38.875069    8.875069\n",
    "18229                   134               135.221466    1.221466\n",
    "\n",
    "Classes used: 2 | Mean Squared Error: 2253.07 | R-squared: 0.16\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                47.526028    8.526028\n",
    "30546                   105                38.257729  -66.742271\n",
    "20806                    35                47.526028   12.526028\n",
    "39438                    30                47.526028   17.526028\n",
    "18229                   134                97.894379  -36.105621"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d79099",
   "metadata": {},
   "source": [
    "Results from ANN pretuning\n",
    "\n",
    "Classes used: 14 | Mean Squared Error: 0.88 | R-squared: 1.00\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                39.270248    0.270248\n",
    "30546                   105               105.376823    0.376823\n",
    "20806                    35                35.337696    0.337696\n",
    "39438                    30                30.255575    0.255575\n",
    "18229                   134               134.399765    0.399765\n",
    "\n",
    "Classes used: 11 | Mean Squared Error: 10.60 | R-squared: 1.00\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                39.504517    0.504517\n",
    "30546                   105               105.519890    0.519890\n",
    "20806                    35                34.829407   -0.170593\n",
    "39438                    30                30.320866    0.320866\n",
    "18229                   134               135.448410    1.448410\n",
    "\n",
    "Classes used: 8 | Mean Squared Error: 214.65 | R-squared: 0.92\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                36.863304   -2.136696\n",
    "30546                   105               114.073128    9.073128\n",
    "20806                    35                33.848129   -1.151871\n",
    "39438                    30                28.835672   -1.164328\n",
    "18229                   134               129.458054   -4.541946\n",
    "\n",
    "Classes used: 5 | Mean Squared Error: 700.22 | R-squared: 0.74\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                48.857971    9.857971\n",
    "30546                   105               118.195877   13.195877\n",
    "20806                    35                56.994919   21.994919\n",
    "39438                    30                37.618988    7.618988\n",
    "18229                   134               140.749390    6.749390\n",
    "\n",
    "Classes used: 2 | Mean Squared Error: 2245.45 | R-squared: 0.16\n",
    "       Actual_TOTAL_LESSONS  Predicted_TOTAL_LESSONS  DIFFERENCE\n",
    "20075                    39                45.878181    6.878181\n",
    "30546                   105                40.134293  -64.865707\n",
    "20806                    35                45.878181   10.878181\n",
    "39438                    30                45.878181   15.878181\n",
    "18229                   134               100.031418  -33.968582"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22121ab",
   "metadata": {},
   "source": [
    "Summary of tuning approach for ANN and benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04da78",
   "metadata": {},
   "source": [
    "Original ANN (Pre-tuning) vs. Post-tuned Model:\n",
    "Classes used: 14\n",
    "\n",
    "Original MSE: 0.88 | Post-tuning MSE: 0.82\n",
    "Original R-squared: 1.00 | Post-tuning R-squared: 1.00\n",
    "Differences:\n",
    "The MSE improved slightly (from 0.88 to 0.82), showing that tuning the hyperparameters made a small improvement.\n",
    "Prediction errors reduced slightly (e.g., row 20075 from 0.27 to 0.05).\n",
    "Overall, this is an incremental improvement in prediction accuracy.\n",
    "Classes used: 11\n",
    "\n",
    "Original MSE: 10.60 | Post-tuning MSE: 8.21\n",
    "Original R-squared: 1.00 | Post-tuning R-squared: 1.00\n",
    "Differences:\n",
    "The MSE improved from 10.60 to 8.21, indicating better accuracy.\n",
    "Some predictions shifted slightly closer to the actual values (e.g., row 30546 from 0.52 to -0.75), reflecting a reduction in error.\n",
    "Classes used: 8\n",
    "\n",
    "Original MSE: 214.65 | Post-tuning MSE: 212.28\n",
    "Original R-squared: 0.92 | Post-tuning R-squared: 0.92\n",
    "Differences:\n",
    "Very small improvement in MSE (from 214.65 to 212.28).\n",
    "Prediction errors for certain classes reduced (e.g., row 39438, from -1.16 to 0.93), but others remained large (e.g., row 30546 with 9.07 to 12.91).\n",
    "Classes used: 5\n",
    "\n",
    "Original MSE: 700.22 | Post-tuning MSE: 702.24\n",
    "Original R-squared: 0.74 | Post-tuning R-squared: 0.74\n",
    "Differences:\n",
    "MSE remained essentially the same, with a very small increase (from 700.22 to 702.24).\n",
    "Errors for certain classes remained high (e.g., row 20075 increased from 9.85 to 11.27).\n",
    "Classes used: 2\n",
    "\n",
    "Original MSE: 2245.45 | Post-tuning MSE: 2253.07\n",
    "Original R-squared: 0.16 | Post-tuning R-squared: 0.16\n",
    "Differences:\n",
    "MSE worsened slightly (from 2245.45 to 2253.07), reflecting no meaningful improvement.\n",
    "Predictions still far from actual values (e.g., row 30546 error worsened from -64.86 to -66.74).\n",
    "Overall Comparison:\n",
    "Significant Improvements:\n",
    "\n",
    "Classes used: 14 and 11: Clear improvements in MSE and reduction in prediction errors. Post-tuning, these models perform more accurately than before.\n",
    "Marginal Changes:\n",
    "\n",
    "Classes used: 8: Minimal improvement. While the MSE decreased slightly, prediction errors remain large for some rows.\n",
    "No Improvement or Regression:\n",
    "\n",
    "Classes used: 5 and 2: These models saw no real improvement and, in some cases, worsened slightly after hyperparameter tuning. The MSE remained high, and errors in prediction stayed large.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "Hyperparameter tuning led to marginal improvements for larger class sets (14, 11), but the performance gains diminish significantly as the number of classes reduces (8, 5, 2). The overall impact of tuning was positive for complex cases but insufficient for models trained with fewer class data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f45d5",
   "metadata": {},
   "source": [
    "Overall comparison of models used and best approach:\n",
    "\n",
    "1. Classes Used: 8\n",
    "Model\tMean Squared Error (MSE)\tR-squared\tComments\n",
    "Linear\t251.91\t0.91\tDecent performance but struggles with high deviations.\n",
    "Random Forest\t239.60\t0.91\tSimilar to linear; moderate prediction accuracy.\n",
    "XGBoost\t224.35\t0.92\tGood performance with lower MSE and slightly better R².\n",
    "ANN Tuned\t212.28\t0.92\tBest performance; captures patterns better with low MSE.\n",
    "\n",
    "Best Model: ANN Tuned\n",
    "Reason: It has the lowest MSE (212.28) with the same R² (0.92) as XGBoost, indicating a slight edge over other models.\n",
    "\n",
    "2. Classes Used: 5\n",
    "Model\tMean Squared Error (MSE)\tR-squared\tComments\n",
    "Linear\t917.67\t0.66\tHigh error, poor performance, large deviations.\n",
    "Random Forest\t784.51\t0.71\tModerate improvement over linear, but still high errors.\n",
    "XGBoost\t706.84\t0.74\tBetter than Random Forest, though still substantial errors.\n",
    "ANN Tuned\t702.24\t0.74\tMarginally the best; lowest MSE but similar to XGBoost.\n",
    "\n",
    "Best Model: ANN Tuned\n",
    "Reason: Though the difference between ANN and XGBoost is marginal, ANN has the lowest MSE (702.24), giving it a slight advantage for predicting in limited data cases.\n",
    "\n",
    "3. Classes Used: 2\n",
    "Model\tMean Squared Error (MSE)\tR-squared\tComments\n",
    "Linear\t2370.54\t0.11\tPoor performance with significant errors.\n",
    "Random Forest\t2290.42\t0.14\tSlightly better than linear, but still very high error.\n",
    "XGBoost\t2239.14\t0.16\tBest among simpler models but still substantial errors.\n",
    "ANN Tuned\t2253.07\t0.16\tSimilar to XGBoost but slightly higher MSE.\n",
    "\n",
    "Best Model: XGBoost\n",
    "Reason: While both XGBoost and ANN are close, XGBoost has the lowest MSE (2239.14) and the highest R² (0.16), making it slightly better for very limited data.\n",
    "\n",
    "Summary of Best Models for Limited Data\n",
    "Classes Used: 8: ANN Tuned performs the best.\n",
    "Classes Used: 5: ANN Tuned edges out other models with the lowest MSE.\n",
    "Classes Used: 2: XGBoost is slightly better than ANN in this very limited data scenario.\n",
    "\n",
    "Overall approaxh:\n",
    "ANN Tuned is model of choice for Classes Used: 8 and 5, as it performs better with low MSE and good R².\n",
    "For Classes Used: 2, XGBoost performs marginally better, but the difference with ANN is minor. Either model could be acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6964b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
